{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\project\\Customer Churn Related Things\\customer_churn_prediction\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import re\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,recall_score,f1_score,precision_score\n",
    "from optimization.ensemble_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = Path('.env')\n",
    "load_dotenv(env_path)\n",
    "\n",
    "root_dir = Path(os.getenv('ROOT_DIRECTORY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_featured_train_set = pd.read_csv(root_dir/'data'/'processed'/\"transformed_featured_train_set.csv\")\n",
    "transformed_featured_val_set = pd.read_csv(root_dir/'data'/'processed'/\"transformed_featured_val_set.csv\")\n",
    "transformed_featured_test_set = pd.read_csv(root_dir/'data'/'processed'/\"transformed_featured_test_set.csv\")\n",
    "transformed_featured_final_train_set = pd.read_csv(root_dir/'data'/'processed'/\"transformed_featured_final_train_set.csv\")\n",
    "lgb_featured_study = pd.read_csv(root_dir/'reports'/\"optemization-study-reports/lgb_featured_study.csv\")\n",
    "xgb_featured_study = pd.read_csv(root_dir/'reports'/\"optemization-study-reports/xgb_featured_study.csv\")\n",
    "catboost_featured_study = pd.read_csv(root_dir/'reports'/\"optemization-study-reports/catboost_featured_study.csv\")\n",
    "nn_featured_study = pd.read_csv(root_dir/'reports'/\"optemization-study-reports/nn_featured_study.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes several functions that handle tasks such as dropping unnecessary columns from a DataFrame, removing json words from feature naems, generating predictions using multiple models, performing soft and weighted voting for ensemble methods, and cleaning hyperparameters. Additionally, it contains a function to create and compile a neural network model using TensorFlow/Keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details, refer to the [documentation](https://custmer-churn-prediction.readthedocs.io/en/latest/source/src.optimization.html) or explore the [source code](https://custmer-churn-prediction.readthedocs.io/en/latest/_modules/src/optimization/ensemble_utils.html#clean_hyperparameters) of this functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    X_train, y_train = transformed_featured_train_set.drop('Churn', axis=1), transformed_featured_train_set['Churn']  # Extract training features and labels\n",
    "    X_val, y_val = transformed_featured_val_set.drop('Churn', axis=1), transformed_featured_val_set['Churn']  # Extract validation features and labels\n",
    "    \n",
    "    # Sample indices for hyperparameters from different models\n",
    "    lgb_study = drop_unnessesary_columns(lgb_featured_study.copy())  # Prepare LightGBM study data\n",
    "    xgb_study = drop_unnessesary_columns(xgb_featured_study.copy())  # Prepare XGBoost study data\n",
    "    cat_study = drop_unnessesary_columns(catboost_featured_study.copy())  # Prepare CatBoost study data\n",
    "    nn_study = drop_unnessesary_columns(nn_featured_study.copy())  # Prepare Neural Network study data\n",
    "    \n",
    "    lgb_idx = trial.suggest_int('lgb_idx', 0, len(lgb_study) - 1)  # Sample index for LightGBM hyperparameters\n",
    "    xgb_idx = trial.suggest_int('xgb_idx', 0, len(xgb_study) - 1)  # Sample index for XGBoost hyperparameters\n",
    "    cat_idx = trial.suggest_int('cat_idx', 0, len(cat_study) - 1)  # Sample index for CatBoost hyperparameters\n",
    "    nn_idx = trial.suggest_int('nn_idx', 0, len(nn_study) - 1)  # Sample index for Neural Network hyperparameters\n",
    "    \n",
    "    # Fetch hyperparameters and clean them\n",
    "    lgb_params = clean_hyperparameters(lgb_study.iloc[lgb_idx].to_dict())  # Get LightGBM parameters\n",
    "    xgb_params = clean_hyperparameters(xgb_study.iloc[xgb_idx].to_dict())  # Get XGBoost parameters\n",
    "    cat_params = clean_hyperparameters(cat_study.iloc[cat_idx].to_dict())  # Get CatBoost parameters\n",
    "    nn_params = clean_hyperparameters(nn_study.iloc[nn_idx].to_dict())  # Get Neural Network parameters\n",
    "    \n",
    "    # Set additional fixed hyperparameters\n",
    "    lgb_params['verbose'] = -1  # Silence LightGBM output\n",
    "    xgb_params['verbose'] = 0  # Silence XGBoost output\n",
    "    cat_params['early_stopping_rounds'] = 3000  # Set early stopping rounds for CatBoost\n",
    "    cat_params['iterations'] = 200  # Set number of iterations for CatBoost\n",
    "\n",
    "    # Weights for the voting classifier\n",
    "    lgb_weight = trial.suggest_float('lgb_weight', 0.1, 1.0)  # Suggest weight for LightGBM\n",
    "    xgb_weight = trial.suggest_float('xgb_weight', 0.1, 1.0)  # Suggest weight for XGBoost\n",
    "    cat_weight = trial.suggest_float('cat_weight', 0.1, 1.0)  # Suggest weight for CatBoost\n",
    "    nn_weight = trial.suggest_float('nn_weight', 0.1, 1.0)  # Suggest weight for Neural Network\n",
    "\n",
    "    weights = {\n",
    "        'lgb': lgb_weight,\n",
    "        'xgb': xgb_weight,\n",
    "        'cat': cat_weight,\n",
    "        'nn': nn_weight\n",
    "    }  # Store weights in a dictionary\n",
    "    \n",
    "    #Let’s give these models some workout time. First up, LightGBM. \n",
    "    #It's like the gym but for data—let’s see if it can lift those predictions high!\n",
    "    \n",
    "    # Train and predict with LightGBM\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train)  # Prepare LightGBM dataset\n",
    "    lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100)  # Train LightGBM model\n",
    "    lgb_preds = lgb_model.predict(X_val)  # Predict on validation set with LightGBM\n",
    "    \n",
    "    #Phew, LightGBM is done. Now let’s see if XGBoost can boost our mood... or just our predictions!\"\n",
    "    \n",
    "    # Train and predict with XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(**xgb_params)  # Initialize XGBoost model with parameters\n",
    "    xgb_model.fit(X_train, y_train)  # Train XGBoost model\n",
    "    xgb_preds = xgb_model.predict_proba(X_val)[:, 1]  # Predict on validation set with XGBoost and get probabilities\n",
    "\n",
    "    # Train and predict with CatBoost\n",
    "    cat_model = cb.CatBoostClassifier(**cat_params, verbose=0)  # Initialize CatBoost model with parameters\n",
    "    cat_model.fit(X_train, y_train)  # Train CatBoost model\n",
    "    cat_preds = cat_model.predict_proba(X_val)[:, 1]  # Predict on validation set with CatBoost and get probabilities\n",
    "\n",
    "    # Train and predict with Neural Network using TensorFlow/Keras\n",
    "    nn_model = create_nn_model(nn_params, transformed_featured_train_set.shape[1] - 1)  # Create Neural Network model\n",
    "    nn_model.fit(X_train, y_train, epochs=50, batch_size=int(nn_params['batch_size']), verbose=0)  # Train Neural Network model\n",
    "    nn_preds = nn_model.predict(transformed_featured_val_set.drop('Churn', axis=1)).ravel()  # Predict on validation set with Neural Network\n",
    "\n",
    "    # Combine predictions using weighted soft voting\n",
    "    predictions = {\n",
    "        'lgb': lgb_preds,\n",
    "        'xgb': xgb_preds,\n",
    "        'cat': cat_preds,\n",
    "        'nn': nn_preds\n",
    "    }  # Store predictions in a dictionary\n",
    "    combined_preds = weighted_voting(predictions, weights)  # Perform weighted voting to combine predictions\n",
    "    preds_digits = [1 if pred >= 0.4 else 0 for pred in combined_preds]  # Convert probabilities to binary predictions with a threshold of 0.4\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    roc_auc = roc_auc_score(y_val, combined_preds)  # Calculate ROC AUC score\n",
    "    f1 = f1_score(y_val, preds_digits)  # Calculate F1 score\n",
    "    recall = recall_score(y_val, preds_digits)  # Calculate recall score\n",
    "    accuracy = accuracy_score(y_val, preds_digits)  # Calculate accuracy score\n",
    "    weighted_recall = 0.65 * recall + 0.35 * f1  # Calculate weighted recall combining recall and F1 score\n",
    "    prec = precision_score(y_val, preds_digits)  # Calculate precision score\n",
    "    \n",
    "    # Store metrics as trial user attributes\n",
    "    trial.set_user_attr('roc', roc_auc)  # Store ROC AUC score in the study\n",
    "    trial.set_user_attr('f1', f1)  # Store F1 score in the study object\n",
    "    trial.set_user_attr('accuracy', accuracy)  # Store accuracy score\n",
    "    trial.set_user_attr('recall', recall)  # Store recall score\n",
    "    trial.set_user_attr('precision', prec)  # Store precision score\n",
    "    \n",
    "    return weighted_recall  # Return weighted recall as the objective value for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ensemble_trials = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-15 10:56:46,694] A new study created in memory with name: no-name-c101482c-5a77-47fd-9904-dbeed6c8ea52\n",
      "c:\\Users\\DELL\\Desktop\\project\\Customer Churn Related Things\\customer_churn_prediction\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\DELL\\Desktop\\project\\Customer Churn Related Things\\customer_churn_prediction\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\DELL\\Desktop\\project\\Customer Churn Related Things\\customer_churn_prediction\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m21/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-15 10:57:28,286] Trial 1 finished with value: 0.6215096492632928 and parameters: {'lgb_idx': 23, 'xgb_idx': 65, 'cat_idx': 25, 'nn_idx': 51, 'lgb_weight': 0.7667207259797195, 'xgb_weight': 0.6911368303740687, 'cat_weight': 0.5654511055100602, 'nn_weight': 0.8822558925247341}. Best is trial 1 with value: 0.6215096492632928.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-15 10:57:28,464] Trial 4 finished with value: 0.6512349890829694 and parameters: {'lgb_idx': 73, 'xgb_idx': 20, 'cat_idx': 34, 'nn_idx': 91, 'lgb_weight': 0.6227081618951118, 'xgb_weight': 0.374496798063199, 'cat_weight': 0.39940266770249566, 'nn_weight': 0.3110658725823954}. Best is trial 4 with value: 0.6512349890829694.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-15 10:57:28,987] Trial 3 finished with value: 0.701046515856852 and parameters: {'lgb_idx': 94, 'xgb_idx': 43, 'cat_idx': 15, 'nn_idx': 18, 'lgb_weight': 0.8727562641518636, 'xgb_weight': 0.45421573635721246, 'cat_weight': 0.6905016923974618, 'nn_weight': 0.7257752178471748}. Best is trial 3 with value: 0.701046515856852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-15 10:57:29,388] Trial 0 finished with value: 0.5743138227513227 and parameters: {'lgb_idx': 94, 'xgb_idx': 14, 'cat_idx': 3, 'nn_idx': 73, 'lgb_weight': 0.3427835329640845, 'xgb_weight': 0.8540210321764163, 'cat_weight': 0.18350122730961033, 'nn_weight': 0.5928577176619084}. Best is trial 3 with value: 0.701046515856852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-15 10:57:32,284] Trial 2 finished with value: 0.681779968175389 and parameters: {'lgb_idx': 81, 'xgb_idx': 43, 'cat_idx': 14, 'nn_idx': 37, 'lgb_weight': 0.37321090103853405, 'xgb_weight': 0.1445777197476065, 'cat_weight': 0.8621837348745022, 'nn_weight': 0.5971281428153259}. Best is trial 3 with value: 0.701046515856852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best metric: 0.701046515856852\n",
      "Best hyperparameters and weights: {'lgb_idx': 94, 'xgb_idx': 43, 'cat_idx': 15, 'nn_idx': 18, 'lgb_weight': 0.8727562641518636, 'xgb_weight': 0.45421573635721246, 'cat_weight': 0.6905016923974618, 'nn_weight': 0.7257752178471748}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if run_ensemble_trials:\n",
    "    # Create Optuna study and optimize\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=5 , n_jobs =-1)\n",
    "\n",
    "    # Best trial\n",
    "    best_trial = study.best_trial\n",
    "    print(f'Best metric: {best_trial.value}')\n",
    "    print('Best hyperparameters and weights:', best_trial.params)\n",
    "    trials = study.trials\n",
    "\n",
    "    # Extract trial data\n",
    "    data = {\n",
    "        'trial_number': [trial.number for trial in trials],\n",
    "        'value': [trial.value for trial in trials],\n",
    "        'params': [trial.params for trial in trials],\n",
    "        'datetime_start': [trial.datetime_start for trial in trials],\n",
    "        'datetime_complete': [trial.datetime_complete for trial in trials],\n",
    "        'f1': [trial.user_attrs.get('f1', None) for trial in trials],\n",
    "        'accuracy': [trial.user_attrs.get('accuracy', None) for trial in trials],\n",
    "        'roc': [trial.user_attrs.get('roc', None) for trial in trials],\n",
    "        'recall': [trial.user_attrs.get('recall', None) for trial in trials],\n",
    "        'precision': [trial.user_attrs.get('precision', None) for trial in trials]\n",
    "        \n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    ensemble_results_df = pd.DataFrame(data)\n",
    "    \n",
    "else:\n",
    "    ensemble_results_df = pd.read_csv(root_dir/'reports'/'optemization-study-reports'/\"ensemble_study.csv\")\n",
    "    ensemble_results_df = ensemble_results_df.drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial_number                                                         3\n",
       "value                                                         0.701047\n",
       "params               {'lgb_idx': 94, 'xgb_idx': 43, 'cat_idx': 15, ...\n",
       "datetime_start                              2024-09-15 10:56:46.742975\n",
       "datetime_complete                           2024-09-15 10:57:28.979025\n",
       "f1                                                            0.659878\n",
       "accuracy                                                        0.8026\n",
       "roc                                                           0.846879\n",
       "recall                                                        0.723214\n",
       "precision                                                     0.606742\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model's performance which has highest weighted recall\n",
    "ensemble_results_df.iloc[ensemble_results_df['value'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial_number                                                         3\n",
       "value                                                         0.701047\n",
       "params               {'lgb_idx': 94, 'xgb_idx': 43, 'cat_idx': 15, ...\n",
       "datetime_start                              2024-09-15 10:56:46.742975\n",
       "datetime_complete                           2024-09-15 10:57:28.979025\n",
       "f1                                                            0.659878\n",
       "accuracy                                                        0.8026\n",
       "roc                                                           0.846879\n",
       "recall                                                        0.723214\n",
       "precision                                                     0.606742\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model's performance which has highest recall\n",
    "ensemble_results_df.iloc[ensemble_results_df['recall'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trial_number                                                         4\n",
       "value                                                         0.651235\n",
       "params               {'lgb_idx': 73, 'xgb_idx': 20, 'cat_idx': 34, ...\n",
       "datetime_start                              2024-09-15 10:56:46.759202\n",
       "datetime_complete                           2024-09-15 10:57:28.464413\n",
       "f1                                                            0.641921\n",
       "accuracy                                                      0.806147\n",
       "roc                                                           0.845229\n",
       "recall                                                         0.65625\n",
       "precision                                                     0.628205\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model's performance which has highest precision\n",
    "ensemble_results_df.iloc[ensemble_results_df['precision'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\Desktop\\project\\Customer Churn Related Things\\customer_churn_prediction\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - AUC: 0.7316 - loss: 0.5184 - val_AUC: 0.8348 - val_loss: 0.4262\n",
      "Epoch 2/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8316 - loss: 0.4374 - val_AUC: 0.8418 - val_loss: 0.4225\n",
      "Epoch 3/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8276 - loss: 0.4370 - val_AUC: 0.8441 - val_loss: 0.4190\n",
      "Epoch 4/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8462 - loss: 0.4137 - val_AUC: 0.8438 - val_loss: 0.4188\n",
      "Epoch 5/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.8473 - loss: 0.4075 - val_AUC: 0.8431 - val_loss: 0.4191\n",
      "Epoch 6/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8503 - loss: 0.4046 - val_AUC: 0.8426 - val_loss: 0.4185\n",
      "Epoch 7/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.8536 - loss: 0.4017 - val_AUC: 0.8434 - val_loss: 0.4180\n",
      "Epoch 8/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8443 - loss: 0.4040 - val_AUC: 0.8446 - val_loss: 0.4216\n",
      "Epoch 9/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8526 - loss: 0.4051 - val_AUC: 0.8453 - val_loss: 0.4169\n",
      "Epoch 10/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8557 - loss: 0.4027 - val_AUC: 0.8450 - val_loss: 0.4155\n",
      "Epoch 11/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8630 - loss: 0.3889 - val_AUC: 0.8433 - val_loss: 0.4207\n",
      "Epoch 12/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8558 - loss: 0.4074 - val_AUC: 0.8428 - val_loss: 0.4170\n",
      "Epoch 13/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8611 - loss: 0.3960 - val_AUC: 0.8439 - val_loss: 0.4168\n",
      "Epoch 14/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8640 - loss: 0.3907 - val_AUC: 0.8418 - val_loss: 0.4183\n",
      "Epoch 15/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.8545 - loss: 0.4075 - val_AUC: 0.8412 - val_loss: 0.4195\n",
      "Epoch 16/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.8686 - loss: 0.3865 - val_AUC: 0.8443 - val_loss: 0.4162\n",
      "Epoch 17/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8632 - loss: 0.3884 - val_AUC: 0.8417 - val_loss: 0.4188\n",
      "Epoch 18/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - AUC: 0.8567 - loss: 0.4078 - val_AUC: 0.8381 - val_loss: 0.4239\n",
      "Epoch 19/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8686 - loss: 0.3908 - val_AUC: 0.8410 - val_loss: 0.4196\n",
      "Epoch 20/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - AUC: 0.8745 - loss: 0.3800 - val_AUC: 0.8404 - val_loss: 0.4202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x28d03fb2050>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_model = create_nn_model(clean_hyperparameters(nn_featured_study.iloc[87].to_dict()),input_shape=transformed_featured_train_set.drop(columns='Churn').shape[1])\n",
    "ann_model.fit(transformed_featured_final_train_set.drop(columns='Churn'),transformed_featured_final_train_set['Churn'],epochs=20,validation_data=(transformed_featured_test_set.drop(columns='Churn'),transformed_featured_test_set['Churn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "roc_auc score is:- 0.8402335374202382\n",
      "recall score is :- 0.6631016042780749\n",
      "precision score is:- 0.5821596244131455\n"
     ]
    }
   ],
   "source": [
    "#let's see final model's performance\n",
    "preds = ann_model.predict(transformed_featured_test_set.drop(columns='Churn'))\n",
    "print('roc_auc score is:-',roc_auc_score(transformed_featured_test_set['Churn'],preds))\n",
    "preds_digits = [1 if pred >= 0.4 else 0 for pred in preds]\n",
    "print('recall score is :-',recall_score(transformed_featured_test_set['Churn'],preds_digits))\n",
    "print('precision score is:-',precision_score(transformed_featured_test_set['Churn'],preds_digits))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
