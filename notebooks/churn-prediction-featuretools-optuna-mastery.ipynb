{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18858,"sourceType":"datasetVersion","datasetId":13996},{"sourceId":9080924,"sourceType":"datasetVersion","datasetId":5478666},{"sourceId":9099748,"sourceType":"datasetVersion","datasetId":5491701},{"sourceId":9108017,"sourceType":"datasetVersion","datasetId":5496885},{"sourceId":9222885,"sourceType":"datasetVersion","datasetId":5577662}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Series Introduction:\n\nWelcome to the **Kaggle Customer Churn Master Series**, where we guide you from the fundamentals to advanced strategies in customer churn prediction. In the [**first notebook**](https://www.kaggle.com/code/deepsutariya/explore-churn-insights-plotly-eda-for-beginners), we laid the groundwork with a thorough Exploratory Data Analysis (EDA), uncovering hidden patterns and preparing our dataset for the rigorous tasks ahead. Now, in the **second notebook** of this series, we're set to transcend the basics, diving into the complexities of ***automated feature engineering using featuretools, model optimization using optuna***, and tailored techniques that push the boundaries of churn prediction.each step is crafted to extract the maximum performance from state-of-the-art models like ***LightGBM, XGBoost, CatBoost , and ANN***.\n","metadata":{}},{"cell_type":"markdown","source":"### Navigation:\n\n- **Previous Notebook:** [Explore Churn Insights: Plotly EDA for Beginners](https://www.kaggle.com/code/deepsutariya/explore-churn-insights-plotly-eda-for-beginners)\n- **Next Notebook:** [Churn Modeling to Deployment: MLflow & DagsHub](https://www.kaggle.com/code/deepsutariya/churn-modeling-to-deployment-mlflow-DagsHub)\n---\n\n### Series Navigation:\n\n- **First Notebook:** [Explore Churn Insights: Plotly EDA for Beginners](https://www.kaggle.com/code/deepsutariya/explore-churn-insights-plotly-eda-for-beginners)\n- **Second Notebook:** [Churn Prediction Featuretools Optuna Mastery](https://www.kaggle.com/code/deepsutariya/churn-prediction-featuretools-optuna-mastery)\n- **Third Notebook:** [Churn Modeling to Deployment: MLflow & DagsHub](https://www.kaggle.com/code/deepsutariya/churn-modeling-to-deployment-mlflow-DagsHub)\n","metadata":{}},{"cell_type":"markdown","source":"### Comprehensive Briefing:\nIn this notebook, I dive deep into the complexities of customer churn prediction by moving beyond traditional approaches. Instead of relying solely on common metrics and methods, I experimented with advanced techniques that often go overlooked. **As we all know , Feature Engineering is The most important part of the ML life cycle**. My approach leveraged automated feature engineering using the Featuretools library, allowing me to extract meaningful insights without needing telecom-specific expertise.\n\nI meticulously optimized multiple state-of-the-art models, including LightGBM, XGBoost, and ANN, using Optuna for hyperparameter tuning. While many focus on ensembling, I found success in fine-tuning individual models and ultimately selecting the best performer, rather than defaulting to a weighted ensemble.\n\nIn a unique twist, I incorporated linear regression to explore hyperparameter relationships, aiming to predict combinations that could deliver exceptional performance. Although not all strategies were successful‚Äîlike SMOTEd datasets and ROC AUC as a primary metric‚ÄîI identified and honed in on the techniques that truly made a difference. This notebook is a comprehensive exploration of what works and what doesn‚Äôt, offering valuable lessons for anyone looking to push the boundaries of churn prediction modeling.","metadata":{"_uuid":"2e8c7d6f-6b53-4cca-a8aa-afecd2b5b5af","_cell_guid":"18d3917d-1232-4e7a-8a9b-22ba0003c0ac","_kg_hide-input":false,"_kg_hide-output":true,"trusted":true}},{"cell_type":"markdown","source":"# Detailed Summary and Rationale\n\n\n### Data Loading and Preprocessing\n\n**DataLoader Class:**\n- **Purpose:** Efficiently handle data loading and preprocessing tasks.\n- **Details:** The `DataLoader` class initializes with the file path of the dataset, loads the data, preprocesses columns, and converts categorical variables to dummy variables.\n- **Rationale:** Automating these tasks ensures consistency and reduces the risk of errors. This modular approach makes the code more maintainable and reusable.\n\n### Train-Test-Validation Split\n\n**Stratified Sampling:**\n- **Purpose:** Maintain the class distribution of the target variable across training, testing, and validation sets.\n- **Details:** The data is split using stratified sampling to ensure that each set has a balanced representation of both churn and non-churn classes.\n- **Rationale:** This approach helps in maintaining the performance metrics' validity and ensures that the model generalizes well across different datasets.\n\n### Handling Class Imbalance\n\n**SMOTE (Synthetic Minority Over-sampling Technique):**\n- **Purpose:** Address the issue of class imbalance in the dataset.\n- **Details:** SMOTE generates synthetic samples for the minority class to balance the class distribution.\n- **Rationale:** Balanced class distribution helps in improving the model's ability to correctly identify both churn and non-churn customers, thereby enhancing the recall and precision of the model.\n\n### Feature Engineering\n\n**Automated Feature Engineering with Featuretools:**\n- **Purpose:** Generate new features automatically from the existing dataset.\n- **Details:** The Featuretools library is used to create meaningful features by capturing relationships and interactions between different variables.\n- **Rationale:** Feature engineering is critical for improving model performance. Automatically generating features can uncover hidden patterns and interactions that might not be immediately obvious.\n\n### Model Training and Selection\n\n**ModelTraining Class:**\n- **Purpose:** Train various machine learning models (LightGBM, CatBoost, XGBoost, Neural Network) and select the best-performing one.\n- **Details:** The `ModelTraining` class includes methods to train different models with early stopping and feature importance calculation.\n- **Rationale:** Using multiple models and selecting the best one based on performance metrics ensures that the final model is robust and well-tuned for the specific task.\n\n### Performance Metrics\n\n**Weighted Recall:**\n- **Purpose:** Evaluate the model's ability to correctly identify the positive class (churn) while considering the class distribution.\n- **Details:** Weighted recall gives more importance to the minority class, ensuring that the model does not ignore it in favor of the majority class.\n- **Rationale:** In churn prediction, correctly identifying churners (the positive class) is crucial for taking preventive actions. Weighted recall helps in optimizing the model for this purpose.\n\n### Optimization\n\n**Optuna for Hyperparameter and Model Selection Optimization:**\n- **Purpose:** Find the optimal hyperparameters and model selection to improve performance.\n- **Details:** Optuna is used to conduct hyperparameter tuning, maximizing metrics such as ROC AUC, F1 score, and recall score. Additionally, Optuna optimizes the weights for the ensemble model.\n- **Rationale:** Hyperparameter optimization and model selection are essential for enhancing model performance and ensuring that the model is well-suited to the data.\n\n### Ensemble Methods\n\n**Optimized Weights for Soft Voting Ensemble:**\n- **Purpose:** Combine the predictions of multiple models to improve overall performance.\n- **Details:** Optuna is used to optimize the weights for combining predictions from LightGBM, CatBoost, XGBoost, and a neural network.\n- **Rationale:** Ensemble methods leverage the strengths of different models, providing a more robust and accurate prediction by reducing individual model biases and variances.\n\n### Linear Regression on Optimized Models\n\n**Fitting Linear Regression:**\n- **Purpose:** Capture the relationships between the hyperparameters and the model performance.\n- **Details:** Linear regression is used to understand and predict how changes in hyperparameters affect model performance. By fitting a linear regression model, we aim to identify hyperparameter combinations that are likely to yield high performance.\n- **Rationale:** This approach helps in fine-tuning the ensemble model by leveraging the insights gained from the linear regression model. It allows for the prediction of hyperparameter sets that are not directly tested but are inferred to perform well based on the regression model.\n\n### Sampler and Checker\n\n**Purpose:**\n- Generate a uniformly distributed dataset of hyperparameters.\n- Use the linear regression weights to predict the performance of these hyperparameters.\n- Apply a performance threshold to select the best models.\n\n**Details:**\n- The sampler creates a uniform distribution of hyperparameter values.\n- The checker uses the linear regression model to predict the performance of these sampled hyperparameters.\n- A performance threshold of 0.635 is applied to filter and select the models.\n- Models that satisfy this threshold are then trained and further evaluated.\n\n**Rationale:**\n- The uniform distribution ensures a comprehensive exploration of the hyperparameter space.\n- The linear regression model helps in quickly estimating the performance of different hyperparameter sets, saving computational resources.\n- Applying a performance threshold ensures that only the most promising models are selected for training, improving overall model efficiency and effectiveness.\n\n","metadata":{"_uuid":"99760c75-9ec1-4c16-b094-6811f7daf622","_cell_guid":"cad1d39a-3df3-45d6-a11a-fba18bb2f663","trusted":true}},{"cell_type":"markdown","source":"## What Didn't Work\n\n- **Ensemble Methods:**\n  - While ensembling is often used to improve model performance, in this case, the ensemble approach did not yield better results compared to individual models. The ensembles failed to outperform the best single model, suggesting that the combined predictions may have introduced additional noise or reduced the effectiveness of the model.\n\n- **SMOTEd Data:**\n  - Applying SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset didn't enhance the model performance as expected. The generated synthetic samples may have introduced patterns that were too similar to existing data, leading to overfitting or not providing the desired generalization.\n\n- **ROC AUC as a Metric:**\n  - The ROC AUC metric, while useful in many scenarios, was not as effective in this particular context. It may not have captured the critical aspects of model performance, especially when the focus was on correctly identifying churners (weighted recall being a better alternative).\n\n- **Linear Regression Fitting:**\n  - Fitting linear regression to model the relationships between hyperparameters and performance didn't produce the expected improvements. The assumptions of linear regression might not have aligned well with the complexity of the hyperparameter interactions, leading to suboptimal predictions.\n\n## What Worked\n\n- **Automated Feature Engineering:**\n  - The use of automated feature engineering, particularly through the Featuretools library, significantly enhanced the model‚Äôs performance. It allowed for the generation of meaningful features without needing deep domain expertise, providing a strong foundation for the models.\n\n- **Optimization of LightGBM:**\n  - Hyperparameter optimization of the LightGBM model using tools like Optuna resulted in substantial performance improvements. The optimized LightGBM model outperformed others, making it a key component of the final solution.","metadata":{"_uuid":"aa4af3db-4cb9-4a38-978d-6331ac81120c","_cell_guid":"5e038152-196d-4660-ba85-4063c289ff29","trusted":true}},{"cell_type":"markdown","source":"#### **Flow diagram of the notebook**","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Path to the image in your local environment\nimage_path = '/kaggle/input/flow-charts/Copy of Algorithm flowchart example(1).jpeg'\n\n# Display the image\ndisplay(Image(filename=image_path))","metadata":{"_uuid":"f1986f16-e4b9-4da1-90b4-c902f9282117","_cell_guid":"61cf2c70-1806-4ab1-ac17-6991e3155e3c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-22T06:21:34.668941Z","iopub.execute_input":"2024-08-22T06:21:34.669384Z","iopub.status.idle":"2024-08-22T06:21:34.701361Z","shell.execute_reply.started":"2024-08-22T06:21:34.669346Z","shell.execute_reply":"2024-08-22T06:21:34.700120Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Variable Descriptions Guide\n\n- **encd_df**: This is the one-hot encoded dataframe used for model training.\n- **val_set**: The validation set used to validate the performance of the models during training.\n- **train_set_splitted**: The remaining part of the training set after splitting out the validation set.\n- **train_set**: The final training set used for training the models.\n- **test_set**: The final test set used to evaluate the performance of the trained models.\n- **X_train_smoted, y_train_smoted**: The training sets after applying SMOTE to handle class imbalance.\n\n### Classes\n\n- **ModelTrainer (class)**: This class is responsible for training different machine learning models.\n- **Feature_Engineering (class)**: This class generates automated features using the `featuretools` library.\n- **FeatureTransformer (class)**: This class is responsible for imputing and scaling the features.\n\n### Transformed Sets\n\n- **transformed_featured_train_set, transformed_featured_val_set**: These are the transformed training and validation sets after feature engineering and transformation.\n- **transformed_featured_final_train_set, transformed_featured_test_set**: The transformed training set (without splitting) and the test set.\n- **transformed_featured_smoted_train_set, transformed_featured_smoted_test_set**: The transformed SMOTEd training and test sets.\n\n### Optimization\n\n- **ModelOptimizer (class)**: This class optimizes the models using Optuna.\n- **featured_lgb_study, featured_xgb_study, featured_cat_study, featured_ann_study**: These are the optimized studies of the models on the feature-engineered sets.\n- **org_lgb_study, org_xgb_study, org_cat_study, org_nn_study**: These are the optimized studies of the models on the original sets (i.e., without feature engineering).\n\n\n### Ensemble Results\n\n- **ensemble_result_df**: The results of the ensemble optimization on weights and models.\n","metadata":{"_uuid":"827233c0-96dd-4e53-a6b8-1fbdced96cac","_cell_guid":"aa8a058f-6507-4965-b42e-a2e8a87eb40c","trusted":true}},{"cell_type":"markdown","source":"**If you have any queries please take a look into the [Q&A](#qa-section) section Or comment on this notebook**","metadata":{"_uuid":"d49e4a30-37cc-48b0-b5b4-3cdfd46b4cdd","_cell_guid":"25f5e83f-ff4b-47f3-ad52-aa5329f947a3","trusted":true}},{"cell_type":"code","source":"# Install necessary libraries\n# featuretools: For automated feature engineering\n# catboost: CatBoost library for gradient boosting\n# optuna: For hyperparameter optimization\n!pip install featuretools \n!pip install catboost\n!pip install optuna","metadata":{"execution":{"iopub.status.idle":"2024-08-21T15:17:56.156653Z","shell.execute_reply.started":"2024-08-21T15:17:09.524532Z","shell.execute_reply":"2024-08-21T15:17:56.155324Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This cell imports all the necessary libraries and modules required \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport featuretools as ft\nimport lightgbm as lgb\nimport xgboost as xgb \nimport catboost as cb\nimport tensorflow as tf\nimport re\nimport optuna\nimport warnings\nimport time\nimport shutil\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler , OrdinalEncoder \nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score,recall_score,precision_score\nfrom sklearn.linear_model import LogisticRegression,LinearRegression\nfrom sklearn.base import BaseEstimator, TransformerMixin ,ClassifierMixin\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom tensorflow.keras.layers import Dense,Input,Flatten,Concatenate,BatchNormalization\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import AUC\nfrom tensorflow.keras.models import load_model","metadata":{"_uuid":"6106148c-3b78-421c-bc5f-af02c11aa2f2","_cell_guid":"b1926b17-6afd-4f59-8df2-7837c23320a6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-21T15:17:56.158236Z","iopub.execute_input":"2024-08-21T15:17:56.158588Z","iopub.status.idle":"2024-08-21T15:18:15.086515Z","shell.execute_reply.started":"2024-08-21T15:17:56.158554Z","shell.execute_reply":"2024-08-21T15:18:15.085075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Suppress all warnings for a cleaner output\n# Set seed for numpy and tensorflow for reproducibility\nwarnings.filterwarnings(\"ignore\")\noptuna.logging.set_verbosity(optuna.logging.CRITICAL)\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"_uuid":"7e861cc9-5796-4969-beb1-854a03cd6dbd","_cell_guid":"a7267022-f7d2-476e-9c82-49d601616432","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-21T15:18:15.089333Z","iopub.execute_input":"2024-08-21T15:18:15.090118Z","iopub.status.idle":"2024-08-21T15:18:15.097441Z","shell.execute_reply.started":"2024-08-21T15:18:15.090081Z","shell.execute_reply":"2024-08-21T15:18:15.095797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{"_uuid":"9c226d03-a0dc-4dcc-a53f-fe8515b205fc","_cell_guid":"de5075e3-7c77-4056-bb2c-c9fda8709fb5","trusted":true}},{"cell_type":"code","source":"# The below class loads the data and performs basic preprocessing steps\n\nclass DataLoader:\n    def __init__(self, file_path):\n        self.file_path = file_path  # Store the file path\n        self.df = None  # Initialize an empty DataFrame\n\n    def load_data(self):\n        # Load data from the CSV file\n        self.df = pd.read_csv(self.file_path)\n        \n        # Replace the 'SeniorCitizen' column values: 0 -> \"No\", 1 -> \"Yes\"\n        self.df['SeniorCitizen'] = self.df['SeniorCitizen'].replace({0: \"No\", 1: \"Yes\"})\n        \n        # Convert 'TotalCharges' to numeric, coercing errors to NaN\n        self.df['TotalCharges'] = pd.to_numeric(self.df['TotalCharges'], errors='coerce')\n        \n        # Replace the 'Churn' column values: \"Yes\" -> 1, \"No\" -> 0\n        self.df['Churn'] = self.df['Churn'].replace({\"Yes\": 1, \"No\": 0})\n        \n        # Drop the 'customerID' column and reset the index\n        self.df = self.df.drop('customerID', axis=1).reset_index()\n        \n        return self.df\n\n    def preprocess_data(self):\n        # Create dummy variables for categorical features (excluding 'Churn')\n        dummy_df = pd.get_dummies(self.df.drop('Churn', axis=1))\n        \n        # Add the 'Churn' column back to the dummy DataFrame\n        dummy_df['Churn'] = self.df['Churn']\n        \n        return dummy_df","metadata":{"_uuid":"6d6a8340-313f-4b33-bfbc-2d785e362cba","_cell_guid":"f1457c43-b79d-420a-9dfe-61106de60cd2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-21T15:18:15.099165Z","iopub.execute_input":"2024-08-21T15:18:15.100056Z","iopub.status.idle":"2024-08-21T15:18:15.135826Z","shell.execute_reply.started":"2024-08-21T15:18:15.100016Z","shell.execute_reply":"2024-08-21T15:18:15.134691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load the data\ndata_loader = DataLoader('/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf = data_loader.load_data()\nencd_df = data_loader.preprocess_data()","metadata":{"_uuid":"99cbaa5f-f110-4f6d-ada3-88b56348d6fe","_cell_guid":"6bcf4a20-fc57-41b2-9890-d17705695307","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-21T15:18:15.137339Z","iopub.execute_input":"2024-08-21T15:18:15.138102Z","iopub.status.idle":"2024-08-21T15:18:15.542458Z","shell.execute_reply.started":"2024-08-21T15:18:15.138066Z","shell.execute_reply":"2024-08-21T15:18:15.541196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"_uuid":"7ef3af16-d9c5-4577-add2-a257d360d20f","_cell_guid":"3fc54443-7176-42b9-9256-72089dd7565e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-21T15:18:15.543903Z","iopub.execute_input":"2024-08-21T15:18:15.544267Z","iopub.status.idle":"2024-08-21T15:18:15.577583Z","shell.execute_reply.started":"2024-08-21T15:18:15.544235Z","shell.execute_reply":"2024-08-21T15:18:15.576428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split into train , test ,validation datasets\ndef split_data(dummy_df):\n    train_set, test_set = train_test_split(dummy_df, test_size=0.2, shuffle=True, random_state=42, stratify=dummy_df['Churn'])\n    train_set_splitted, val_set = train_test_split(train_set, test_size=0.15, shuffle=True, random_state=42, stratify=train_set['Churn'])\n    return train_set, test_set, train_set_splitted, val_set","metadata":{"_uuid":"2db5f7b1-48fe-44fe-bc05-43403eeb83c5","_cell_guid":"bac0ca0b-81a0-4177-bedd-be522c5e1425","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set,test_set,train_set_splitted,val_set = split_data(encd_df.dropna())\nX_train , y_train , X_test , y_test = train_set.drop(columns=['Churn','index']) , train_set['Churn'] , test_set.drop(columns=['index','Churn']) , test_set['Churn']\nX_train_splitted , y_train_splitted, X_val,y_val = train_set_splitted.drop('Churn',axis = 1 ) , train_set_splitted['Churn'] , val_set.drop('Churn',axis = 1) , val_set['Churn']\nX_train_splitted.shape","metadata":{"_uuid":"3c66c355-25b6-4094-b62e-c43526418313","_cell_guid":"56a52f1e-e8fb-41c5-9e94-4a4b3d149449","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there is class imbalance that can affect results\n# To experiment with balance dataset I am using SMOTE algorithm.\nX_train_smoted,y_train_smoted = SMOTE().fit_resample(X_train_splitted,y_train_splitted)","metadata":{"_uuid":"65155251-1085-461e-a743-e807ec59e7c1","_cell_guid":"702d49f8-5783-4567-bd62-c5e0bfe7de91","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smoted_df = X_train_smoted\nsmoted_df['Churn'] = y_train_smoted\nsmoted_df = smoted_df.drop(columns='index').reset_index()","metadata":{"_uuid":"c746d57c-b722-4338-bd60-6d7938b26158","_cell_guid":"3f9947b1-0f60-43a9-b2a0-5c0a701562d8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection and Importance","metadata":{"_uuid":"5b32fe14-0f5d-4c42-9a12-ccb66981faa9","_cell_guid":"69bfff53-017c-47dc-a4d2-99a8dbd182d2","trusted":true}},{"cell_type":"markdown","source":"### First we will train a lgb model. This model will act as base line model.","metadata":{}},{"cell_type":"code","source":"class ModelTrainer:\n    def __init__(self, train_data, test_data, df=None, model_name=None):\n        self.model_name = model_name  # Store the model name\n        self.train_data = train_data  # Training data\n        self.test_data = test_data  # Test data\n        self.df = df  # Additional DataFrame if needed\n\n    def train_lightgbm(self, params=None):\n        # Get feature columns excluding 'Churn'\n        columns = self.train_data.drop('Churn', axis=1).columns\n        \n        # Create LightGBM datasets for training and testing\n        train_data = lgb.Dataset(self.train_data.drop('Churn', axis=1), label=self.train_data['Churn'])\n        test_data = lgb.Dataset(self.test_data.drop('Churn', axis=1), label=self.test_data['Churn'], reference=train_data)\n        \n        # Define default parameters if none are provided\n        if params is None: \n            params = {\n                'objective': 'binary',\n                'boosting_type': 'gbdt',\n                'metric': 'auc',\n                'num_leaves': 31,\n                'learning_rate': 0.05,\n                'feature_fraction': 0.9,\n                'seed': 42,\n                # 'verbose': -1,  # Uncomment if verbose output is needed\n            }\n        \n        evals_result = {}  # Dictionary to store evaluation results\n        callbacks = [lgb.early_stopping(stopping_rounds=200), lgb.record_evaluation(evals_result)]\n        \n        # Train the LightGBM model\n        booster = lgb.train(params, train_data, num_boost_round=3000, valid_sets=[test_data], callbacks=callbacks)\n        \n        # Get the AUC score from the evaluation results\n        auc_score = evals_result['valid_0']['auc'][-1]\n        \n        # Get feature importance\n        feature_importance = booster.feature_importance(importance_type='gain')\n        feature_importance_df = pd.DataFrame({\n            'features': columns,\n            'importance': feature_importance\n        }).sort_values(by='importance', ascending=False)\n        \n        # Add the AUC score to the feature importance DataFrame\n        feature_importance_df['auc'] = auc_score\n        \n        return feature_importance_df, booster\n\n    def train_catboost(self):\n        # Initialize and train the CatBoost classifier\n        cat_model = CatBoostClassifier(verbose=0, iterations=3000)\n        cat_model.fit(self.train_data.drop('Churn', axis=1), self.train_data['Churn'], early_stopping_rounds=200)\n        \n        # Predict probabilities and calculate AUC score\n        cat_preds = cat_model.predict_proba(self.test_data.drop('Churn', axis=1))[:, 1]\n        cat_auc = roc_auc_score(self.test_data['Churn'], cat_preds)\n        \n        print(cat_auc)\n        return cat_auc\n\n    def train_xgboost(self):\n        params = {\n            'objective': 'binary:logistic',\n            'eval_metric': 'auc',\n        }\n        \n        # Create DMatrix for training and evaluation\n        dtrain = xgb.DMatrix(data=self.train_data.drop('Churn', axis=1), label=self.train_data['Churn'])\n        deval = xgb.DMatrix(data=self.test_data.drop('Churn', axis=1), label=self.test_data['Churn'])\n\n        # Define watchlist for evaluation\n        watchlist = [(dtrain, 'train'), (deval, 'eval')]\n\n        # Train the model with early stopping\n        bst = xgb.train(\n            params,\n            dtrain,\n            num_boost_round=3000,  # Maximum number of boosting rounds\n            evals=watchlist,\n            early_stopping_rounds=200,\n            verbose_eval=0  # Suppress verbose output\n        )\n\n        # Make predictions and calculate AUC score\n        xgb_preds = bst.predict(deval)\n        xgb_auc = roc_auc_score(self.test_data['Churn'], xgb_preds)\n        \n        print(xgb_auc)\n        return xgb_auc\n\n    def train_neural_network(self):\n        #  NaN values and unscaled data  can affect the results in ANN.\n        # thus it requires seperate preprocessing\n        self.df = self.df.dropna()  # Drop rows with missing values\n        \n        # Drop 'index' column if it exists\n        if 'index' in self.df.columns:\n            self.df = self.df.drop('index', axis=1).reset_index(drop=True)\n        \n        # Separate categorical and numerical columns\n        cat_list = [i for i in self.df.columns if self.df[i].dtype == 'object']\n        num_list = [i for i in self.df.columns if self.df[i].dtype != 'object']\n        num_list.remove('Churn')\n        \n        # Split the data into training and testing sets\n        train_set, test_set = train_test_split(self.df, test_size=0.2, shuffle=True, random_state=42, stratify=self.df['Churn'])\n        \n        # Define column transformer for preprocessing\n        col_tranfm = ColumnTransformer(\n            transformers=[\n                (\"OneHotEncoder\", OneHotEncoder(), cat_list),\n                (\"StandardScaler\", StandardScaler(), num_list),\n            ],\n            n_jobs=-1,\n            verbose=True,\n            verbose_feature_names_out=True,\n            remainder='passthrough'\n        )\n        \n        # Fit and transform the training set\n        train_set = col_tranfm.fit_transform(train_set)\n        train_set = pd.DataFrame(train_set, columns=col_tranfm.get_feature_names_out())\n        \n        # Transform the test set\n        test_set = col_tranfm.transform(test_set)\n        test_set = pd.DataFrame(test_set, columns=col_tranfm.get_feature_names_out())\n        \n        # Rename the 'Churn' column\n        train_set = train_set.rename(columns={'remainder__Churn': 'Churn'})\n        test_set = test_set.rename(columns={'remainder__Churn': 'Churn'})\n        \n        # Split the training set further into training and validation sets\n        train_set_splitted, val_set = train_test_split(train_set, test_size=0.15, shuffle=True, random_state=42, stratify=train_set['Churn'])\n        \n        # Separate features and target variable for training and validation sets\n        X_train, y_train = train_set.drop('Churn', axis=1), train_set['Churn']\n        X_val, y_val = val_set.drop('Churn', axis=1), val_set['Churn']\n        X_train_splitted, y_train_splitted = train_set_splitted.drop('Churn', axis=1), train_set_splitted['Churn']\n        \n        # Convert data types to float64\n        X_train_splitted = X_train_splitted.astype('float64')\n        y_train_splitted = y_train_splitted.astype('float64')\n        X_val, y_val = X_val.astype('float64'), y_val.astype('float64')\n        \n        callback = WeightLogger()  # Initialize custom callback\n        \n        # Define the neural network architecture\n        model = Sequential()\n        model.add(Dense(64, activation='selu', kernel_initializer='lecun_normal'))\n        model.add(Dropout(0.5))\n        model.add(Dense(32, activation='selu', kernel_initializer='lecun_normal'))\n        model.add(Dropout(0.5))\n        model.add(Dense(32, activation='selu', kernel_initializer='lecun_normal'))\n        model.add(Dense(1, activation='sigmoid'))\n        \n        # Compile the model\n        model.compile(optimizer=\"nadam\", loss=\"binary_crossentropy\", metrics=['accuracy', AUC(name=\"roc_auc\")])\n        \n        # Train the model\n        history = model.fit(X_train_splitted, y_train_splitted, validation_data=(X_val, y_val), verbose=0, epochs=20)\n        \n        return history\n\n    def train(self):\n        if self.model_name is not None:\n            if self.model_name == \"lgb\":\n                return self.train_lightgbm()\n            elif self.model_name == \"cat\":\n                return self.train_catboost()\n            elif self.model_name == \"xgb\":\n                return self.train_xgboost()\n            elif self.model_name == \"nn\":\n                return self.train_neural_network()","metadata":{"_uuid":"0012d8cb-aeb3-4771-892f-69d9e8fa2011","_cell_guid":"ee4c42a7-5ac2-494d-945a-5c3e89b10ea8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_trainer = ModelTrainer(train_set_splitted.drop(columns='index'),val_set.drop(columns='index'))\nfeature_importance_df ,_= model_trainer.train_lightgbm()","metadata":{"_uuid":"2afb41ae-156c-4fe1-aa83-e8ebc909dca1","_cell_guid":"b4350347-6605-4e7b-bfd2-c12c0c412793","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance_df.head(20)","metadata":{"_uuid":"e7001ffd-b718-4e68-9087-c76372d44222","_cell_guid":"a32dda92-34c8-4075-9362-b4d08608cd2b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's examine how the smoted dataset is peroforming\nmodel_trainer = ModelTrainer(smoted_df.drop(columns='index'),val_set.drop(columns='index'))\nfeature_importance_df,_ = model_trainer.train_lightgbm()","metadata":{"_uuid":"44e9e420-4a74-400d-bbda-f31d8f25aaca","_cell_guid":"286533ae-6801-49e4-a8fc-4933cd465791","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# üö® **The Critical Role of Feature Engineering in Machine Learning** üö®\n\nIn the rapidly evolving world of machine learning, one truth remains constant: **the quality of your features determines the quality of your model**. Feature engineering is not just another step in the pipeline; it is the foundation upon which your model's performance is built. Without robust, well-crafted features, even the most sophisticated algorithms can falter. \n\nImagine you're an architect. You could have the most advanced tools and technology at your disposal, but if the building materials you use are subpar, the final structure will inevitably be weak. The same principle applies in machine learning‚Äî**feature engineering is the process of transforming raw data into the critical building blocks that empower your model to achieve excellence and ```without exaggeration it is the most important part in the ML lifecycle```**.\n\n## üõ†Ô∏è **FeatureTools: The Architect of Your Model‚Äôs Success** üõ†Ô∏è\n\nEnter **FeatureTools**‚Äîa powerful library that automates the process of creating meaningful features from your raw data. But make no mistake‚Äîthis is not just automation for convenience's sake. FeatureTools is your blueprint for success, providing a systematic way to craft features that will enable your model to not only learn but to excel in predicting outcomes with precision.Let's learn this serious topic with fun.\n\n### **Why Feature Engineering Matters**\n\n- **Entities**: These are the ingredients‚Äîyour raw datasets‚Äîlike vegetables and spices. In this case, our training and test sets are the ingredients, and just like how each veggie might need to be chopped differently, each dataset needs its own unique index to keep things in order. i.e individual dataframe is named as an entity.\n\n- **EntitySet**: Think of this as your kitchen. It‚Äôs where you store all your ingredients (dataframes). Each dataset, like your training and test sets, gets its own special spot in this kitchen, labeled with unique names so they never get mixed up. i.e Set of dataframe(s) and their relations is called entityset.\n  \n\n\n- **Primitive Transformations**: Ever seen a chef quickly dice an onion into perfect little cubes? That‚Äôs what these transformations do to your data. They take your basic ingredients and slice them, dice them, and mix them in ways that create new, richer flavors‚Äîer, I mean features!. i.e Generating new features from individual features is called ``transformation``.\n\n- **Aggregation**: Picture a large pot where all the diced onions, tomatoes, and spices simmer together. Aggregation takes your data and combines it to extract meaningful patterns. It‚Äôs like boiling down all those ingredients into a rich, flavorful stock‚Äîperfect for adding depth to your model. i.e Generating features from combination of multiple features is called ``Aggregation``.\n\n\n\n## **Building a Model with Purpose**\n\nWhen you use FeatureTools, you're not just building a model‚Äîyou‚Äôre constructing a solution with real-world impact. This isn‚Äôt just theory; **this is the practice of creating features that will enable your model to perform reliably in the most demanding production environments**.\n\nIn our `Feature_Engineering` class, we‚Äôre taking this theory into practice. From setting up your EntitySet to generating and refining features, every step is a deliberate action to enhance your model‚Äôs performance. This is feature engineering with a purpose.\n\n1. **Create_Entityset**: First, we set up our kitchen with the `Create_Entityset` method. We organize our ingredients (datasets) neatly, making sure everything has its place. If our ingredients don‚Äôt come pre-labeled (indexed), our magical helpers create labels for us, ensuring nothing gets lost in the mix. i.e this method creates ``Entityset`` from multiple dataframe(s) and their relations.\n\n2. **add_dataframe**: Using this method we can add dataframe into the entity set.For that  we should provide dataframe_name,datframe,index(if your dataframe hasn't index you have to set make_index=True).\n\n2. **Generate_Features**: Next, we call on FeatureTools to start the real magic‚Äîtransforming our raw ingredients into delicious features. It‚Äôs like watching a chef in action, taking raw veggies and turning them into a gourmet meal. Whether it's chopping (transformations) or slow-cooking (aggregations), our features get enhanced, refined, and prepped for the main event: feeding the model!. i.e this method generates features from raw data and combine it with target dataframe.\n\n3. **Cleaning and Alignment**: But even in the best kitchens, you need to tidy up. So, we clean the names, remove any duplicates, and ensure that everything is aligned perfectly between the training and test sets‚Äîno mismatched spices here!. i.e After feature engineering we will get **mutlilevel indexed** target dataframe. we have to convert it into normal dataframe. \n\n## **Important Terminologies**\n\n### Transformation Primitives\n- **Transformation Primitives** are like little recipes that take one column and create new features by applying mathematical or logical operations. For example, if you have a column with dates, a transformation primitive could create a new column with the day of the week (Monday, Tuesday, etc.).\n\n### Aggregation Primitives\n- **Aggregation Primitives** are used when you have data grouped in some way, like sales data for different stores. These primitives combine the data in each group to create a single summary value, like the average sales per store or the total sales for each month.\n\n### Ignore Columns\n- **Ignore Columns** are columns that you tell the program not to use when creating new features. Maybe these columns contain information that would give away the answer too easily (like a column that directly says whether a customer has left), or maybe they just aren't useful for your model.\n","metadata":{}},{"cell_type":"markdown","source":"For more details visit featuretools [Documentation](https://featuretools.alteryx.com/en/stable/)","metadata":{}},{"cell_type":"code","source":"class Feature_Engineering:\n    def __init__(self, train_set, test_set, df):\n        self.train_set = train_set  # Training dataset\n        self.test_set = test_set  # Test dataset\n        self.entity_set = None  # Placeholder for entity set\n        self.train_set_name = None  # Name of training dataset in entity set\n        self.test_set_name = None  # Name of test dataset in entity set\n        self.df = df  # Original DataFrame\n\n    def Create_Entityset(self, entity_id, train_set_name, test_set_name, index_name=None):\n        # Check if index_name is not present in train and test sets\n        if index_name not in self.train_set.columns and index_name not in self.test_set.columns:\n            es = ft.EntitySet(id=entity_id)\n            # Add train_set to the entity set with make_index=True\n            es.add_dataframe(\n                dataframe_name=train_set_name,\n                dataframe=self.train_set, \n                make_index=True,\n                index=index_name,\n                # time_index='tenure'  # Uncomment if using time index\n            ) \n            # Add test_set to the entity set with make_index=True\n            es.add_dataframe(\n                dataframe_name=test_set_name,\n                dataframe=self.test_set,\n                make_index=True,\n                index=index_name,\n                # time_index='tenure'  # Uncomment if using time index\n            )\n        else:\n            es = ft.EntitySet(id=entity_id)\n            # Add train_set to the entity set with existing index\n            es.add_dataframe(\n                dataframe_name=train_set_name,\n                dataframe=self.train_set,\n                index=index_name,\n                # time_index='tenure'  # Uncomment if using time index\n            )\n            # Add test_set to the entity set with existing index\n            es.add_dataframe(\n                dataframe_name=test_set_name,\n                dataframe=self.test_set,\n                index=index_name,\n                # time_index='tenure'  # Uncomment if using time index\n            )\n        self.entity_set = es  # Store the entity set\n        self.train_set_name = train_set_name  # Store the training set name\n        self.test_set_name = test_set_name  # Store the test set name\n\n    def __clean_feature_names(self, df):\n        # Clean column names by replacing special characters with underscore\n        cleaned_names = []\n        for col in df.columns:\n            clean_name = re.sub(r'[^A-Za-z0-9_]+', '_', col)\n            cleaned_names.append(clean_name)\n        df.columns = cleaned_names\n        return df\n\n    def __remove_duplicate_columns(self, df):\n        # Remove duplicate columns\n        df = df.loc[:, ~df.columns.duplicated()]\n        return df\n\n    def Generate_Features(self, trans_list=None, agg_list=None, ignore_columns=None, names_only=True):\n        if names_only == False:\n            # Generate features for training set\n            # dfs generate features from data and merge them with target dataframe\n            feature_df, feature_names = ft.dfs(\n                entityset=self.entity_set, # for which entity set do you want to generate features.\n                target_dataframe_name=self.train_set_name, # To Which dataframe features must be merged? \n                trans_primitives=trans_list, # list of the transformations that you want to apply .\n                agg_primitives = agg_list, # Aggregation primitives means methods that combine multiple rows like median,average,sum etc.\n                max_depth=2, # Number of  transformation,aggregation  primitves  can be stacked upon each other.\n                ignore_columns=ignore_columns, # columns that shouldn't use for feature engineering . Example 'Churn','index'\n                features_only=names_only, # should it generate features or just provide names of the\n                ignore_dataframes=[self.test_set_name] # which dataframes should avoid to mitigate data leak.\n            )\n            # Ensure 'index' column in feature_df is treated as integer index\n            feature_df = feature_df.reset_index()\n            feature_df['index'] = feature_df['index'].astype(int)\n\n            # Align feature_df index to avoid out-of-range or mismatch\n            aligned_churn = self.df.loc[feature_df['index'], 'Churn'].reset_index(drop=True)\n\n            # Assign aligned Churn values to feature_df\n            feature_df['Churn'] = aligned_churn\n\n            # Generate features for test set\n            feature_df_test, features_test_name = ft.dfs(\n                entityset=self.entity_set,\n                target_dataframe_name=self.test_set_name,\n                trans_primitives=trans_list,\n                max_depth=2,\n                ignore_columns=ignore_columns,\n                features_only=names_only,\n                ignore_dataframes=[self.train_set_name]\n            )\n            # Ensure 'index' column in feature_df_test is treated as integer index\n            feature_df_test = feature_df_test.reset_index()\n            feature_df_test['index'] = feature_df_test['index'].astype(int)\n\n            # Align feature_df_test index to avoid out-of-range or mismatch\n            aligned_churn_test = self.df.loc[feature_df_test['index'], 'Churn'].reset_index(drop=True)\n\n            # Assign aligned Churn values to feature_df_test\n            feature_df_test['Churn'] = aligned_churn_test\n\n            # Drop 'index' column from feature dataframes\n            feature_df = feature_df.drop('index', axis=1)\n            feature_df_test = feature_df_test.drop('index', axis=1)\n\n            # Clean feature names for both train and test datasets\n            feature_df = self.__clean_feature_names(feature_df)\n            feature_df_test = self.__clean_feature_names(feature_df_test)\n\n            # Remove duplicate columns from both datasets\n            feature_df = self.__remove_duplicate_columns(feature_df)\n            feature_df_test = self.__remove_duplicate_columns(feature_df_test)\n\n            # Remove columns with a single unique value\n            single_col_list = [col for col in feature_df.columns if feature_df[col].nunique() == 1]\n            feature_df = feature_df.drop(columns=single_col_list)\n\n            # Replace infinite values with NaN\n            feature_df = feature_df.replace([-np.inf, np.inf], np.nan)\n            feature_df_test = feature_df_test.replace([-np.inf, np.inf], np.nan)\n\n            # Align the datasets to keep only common columns\n            featured_train_labels = feature_df['Churn']\n            feature_df_aligned, feature_df_test_aligned = feature_df.align(feature_df_test, join='inner', axis=1)\n            feature_df_aligned['Churn'] = featured_train_labels\n\n            return feature_df_aligned, feature_df_test_aligned\n        else:\n            # Generate feature names only for training set\n            feature_names = ft.dfs(\n                entityset=self.entity_set,\n                target_dataframe_name=self.train_set_name,\n                trans_primitives=trans_list,\n                max_depth=2,\n                ignore_columns=ignore_columns,\n                features_only=names_only,\n                ignore_dataframes=[self.test_set_name]\n            )\n            # Generate feature names only for test set\n            feature_test_names = ft.dfs(\n                entityset=self.entity_set,\n                target_dataframe_name=self.test_set_name,\n                trans_primitives=trans_list,\n                max_depth=2,\n                ignore_columns=ignore_columns,\n                features_only=names_only,\n                ignore_dataframes=[self.train_set_name]\n            )\n            return feature_names, feature_test_names","metadata":{"_uuid":"bb6cf1bc-32e3-4610-af4e-9fd92ee33d38","_cell_guid":"ca508150-5ad5-4e42-a31f-e00094d64d19","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For simplicity i am not taking any aggeragation primitives.\ntrans_list =  [\n 'multiply_numeric_boolean',\n 'absolute_diff',\n 'email_address_to_domain',\n 'exponential_weighted_variance',\n 'modulo_numeric',\n 'rate_of_change',\n 'url_to_protocol',\n 'greater_than',\n 'multiply_numeric_scalar',\n 'less_than_equal_to',\n 'longitude',\n 'age',\n 'cosine',\n 'subtract_numeric',\n 'week',\n 'cityblock_distance',\n 'rolling_max',\n 'subtract_numeric_scalar',\n 'is_quarter_end',\n 'less_than_scalar',\n 'exponential_weighted_std',\n 'natural_logarithm',\n 'add_numeric_scalar',\n 'percent_change',\n 'subtract_numeric',\n 'is_lunch_time'\n]","metadata":{"_uuid":"4c56d5fd-84f5-4a94-98f4-b62bd5cae578","_cell_guid":"cc62f89a-64e4-4af4-af18-a94619661591","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# while feature generating we shouldn't use Churn and index.\n# Using Churn will cause data leak\n# Index is redundant feature\nignore_columns = {\n    'smoted_train':['Churn','index'],\n    'val_test':['Churn','index',]\n}\nval_copy = val_set.copy()\n\n# let's generate features for the smoted dataset\n\nfeature_eng = Feature_Engineering(smoted_df,val_copy,encd_df)\nfeature_eng.Create_Entityset('smoted','smoted_train','val_test','index')\n\n# the below sets will be used for the evaluation of the smoted datasets\nsmoted_featured_train_set , smoted_featured_test_set = feature_eng.Generate_Features(trans_list,ignore_columns = ignore_columns , names_only=False)\n\n# let's generate features for the splitted_train , val set\nignore_columns = {\n    'train':['Churn','index'],\n    'test':['Churn','index',]\n}\nfeature_eng = Feature_Engineering(train_set_splitted,val_set,encd_df)\nfeature_eng.Create_Entityset('validation','train','test','index')\n\n# the below sets will be used for the training and validation process\nfeatured_train_set , featured_val_set = feature_eng.Generate_Features(trans_list,ignore_columns = ignore_columns , names_only=False)\n\n\n# below code generate the features for the final_train , test set\nignore_columns = {\n    'final_train':['Churn','index'],\n    'final_test':['Churn','index',]\n}\nfeature_eng = Feature_Engineering(train_set,test_set,encd_df)\nfeature_eng.Create_Entityset('final','final_train','final_test','index')\n \n# the below sets will be used for the train and test the final model that we will get from the val set\nfeatured_final_train_set , featured_test_set = feature_eng.Generate_Features(trans_list,ignore_columns = ignore_columns , names_only=False)","metadata":{"_uuid":"ac51d81b-1292-4d3d-93df-c49c5388da8b","_cell_guid":"4dc5c47c-da56-4324-ba39-ea1398952001","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the result is indicating the significant performance boost due to feature engineering\nengineered_feature_importance_df =  ModelTrainer(featured_train_set,featured_val_set).train_lightgbm()","metadata":{"_uuid":"e11819a7-fcdd-45bb-8aa5-e748d38c8f7d","_cell_guid":"e89ea9c8-caac-416b-97b7-3249b77d6256","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**If you find this notebook helpful and inspiring, don‚Äôt forget to give it a thumbs up! üëç**","metadata":{}},{"cell_type":"markdown","source":"# Training and Optimization","metadata":{"_uuid":"228dfc37-cbe4-4c72-80cd-5ba18533bfa0","_cell_guid":"3f40349a-650d-4049-8c23-e44ee6a97578","trusted":true}},{"cell_type":"code","source":"class FeatureTransformer:\n    def __init__(self, train_set, test_set):\n        self.train_set = train_set  # Training dataset\n        self.test_set = test_set  # Test dataset\n\n        # Lists to store numerical and categorical feature names\n        self.num_list = [col for col in train_set.columns if train_set[col].dtype != 'bool']\n        self.cat_list = [col for col in train_set.columns if train_set[col].dtype == 'bool']\n        \n        self.num_list.remove('Churn')  # Remove the target variable from numerical features\n\n        # Define the column transformer\n        self.col_transfm = ColumnTransformer(\n            transformers=[\n                # Pipeline for numerical features: imputation and scaling\n                (\"num\", Pipeline(\n                    steps=[\n                        (\"imputer\", SimpleImputer(strategy='median')),  # Impute missing values with median\n                        (\"scaler\", StandardScaler())  # Standardize numerical features\n                    ]), self.num_list)\n            ],\n            n_jobs=-1,  # Use all available cores\n            verbose=True,  # Verbose output\n            verbose_feature_names_out=True,  # Verbose feature names\n            remainder='passthrough'  # Keep remaining columns as they are\n        )\n\n    def transform(self):\n        # Fit and transform the training set\n        self.train_set = self.col_transfm.fit_transform(self.train_set)\n        # Transform the test set\n        self.test_set = self.col_transfm.transform(self.test_set)\n\n        # Convert transformed data to DataFrames with feature names\n        self.train_set = pd.DataFrame(self.train_set, columns=self.col_transfm.get_feature_names_out())\n        self.test_set = pd.DataFrame(self.test_set, columns=self.col_transfm.get_feature_names_out())\n\n        # Rename the Churn column\n        self.train_set = self.train_set.rename(columns={'remainder__Churn': 'Churn'})\n        self.test_set = self.test_set.rename(columns={'remainder__Churn': 'Churn'})\n\n        # Ensure all data is of type float64\n        self.train_set, self.test_set = self.train_set.astype('float64'), self.test_set.astype('float64')\n\n        return self.train_set, self.test_set  # Return the transformed datasets","metadata":{"_uuid":"758d9237-9fc8-4d5e-87a8-13e45f2c4d49","_cell_guid":"efbce61f-216f-4d8b-bd9c-681ac340d6e1","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the feature engineering pipeline to the training and validation sets\n# - Fit and transform the training set using the pipeline\n# - Transform the validation set using the pipeline\n# - Ensure that the same transformations are applied to both sets\n\nfeature_transformer = FeatureTransformer(featured_train_set,featured_val_set)\ntransformed_featured_train_set , transformed_featured_val_set = feature_transformer.transform()","metadata":{"_uuid":"73b2a0bc-d5e3-423a-8567-077268c39168","_cell_guid":"033e90bb-6c91-4020-ad56-ffae1aea889a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see whether the featured smoted datasets are performing well or not.\nThe AUC score is suggesting that smoted datasets are overfitting thus i am not going to use it any more , now we left with the featured dataset and original dataset","metadata":{"_uuid":"eb0197b1-baee-4c7b-b35d-889badca34de","_cell_guid":"f9828609-942c-4e32-a2b2-29968c8870e4","trusted":true}},{"cell_type":"code","source":"engineered_feature_importance_df =  ModelTrainer(smoted_featured_train_set,smoted_featured_test_set).train_lightgbm()\nengineered_feature_importance_df","metadata":{"_uuid":"620fcbbe-67f6-4f38-a1d5-c60b9e202727","_cell_guid":"86b8205c-3f23-4f35-a750-ec2f7bb3e70d","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transforming final train set and test set\nfeature_transformer = FeatureTransformer(featured_final_train_set,featured_test_set)\ntransformed_featured_final_train_set , transformed_featured_test_set = feature_transformer.transform()","metadata":{"_uuid":"06924c08-866f-421e-8947-c03fe620a80a","_cell_guid":"fe4982e2-ce1b-4691-a59a-43dcc0e250ff","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_transformer = FeatureTransformer(smoted_featured_train_set , smoted_featured_test_set)\ntransformed_featured_smoted_train_set , transformed_featured_smoted_test_set = feature_transformer.transform()","metadata":{"_uuid":"53e64671-5c7f-47d9-87c9-1224839d65d4","_cell_guid":"15913d4e-8bfd-4c84-8907-e28145489302","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Emphasizing the Importance of Optimization and Optuna in Machine Learning\n\nIn the ever-evolving field of Machine Learning (ML), achieving the best possible performance from models is not just a luxury‚Äîit's a necessity. Optimization is the key process that allows us to fine-tune models, ensuring that they perform at their peak across various metrics. Optuna, a powerful hyperparameter optimization framework, plays a crucial role in this endeavor. By systematically searching the parameter space, Optuna helps us find the most effective model configurations, often uncovering improvements that might be missed through manual tuning.\n\nThe pursuit of optimization should be taken very seriously by anyone involved in ML. It‚Äôs not merely about squeezing out the last bit of performance; it‚Äôs about understanding the intricacies of model behavior and the profound impact that well-tuned models can have on real-world applications. As you dive deeper into this code, let this mindset guide you: optimization is not a mere step but a cornerstone of advanced ML practice. Stay curious and serious about mastering this skill, as it will set you apart in the world of data science.\n\n## Key Terms and Their Significance\n\n- **Weighted Recall**: This is a custom metric defined as `0.65 * recall + 0.35 * f1`. The idea behind weighted recall is to combine the recall (sensitivity) and F1 score into a single metric, giving more weight to recall. This is particularly important in cases where false negatives (e.g., not detecting churn when it actually occurs) are more costly than false positives.\n\n- **Optuna**: Optuna is an open-source hyperparameter optimization framework that automates the search for optimal hyperparameters. In this code, Optuna is used to optimize various parameters of different models, such as learning rate, depth, and regularization parameters. Its efficiency comes from advanced sampling techniques like TPE (Tree-structured Parzen Estimator) and pruning algorithms like Hyperband, which help in finding the best model configurations faster.\n\n- **TPE Sampler**: This is a sampling algorithm used by Optuna that models the objective function using a Gaussian Mixture Model. It allows for efficient sampling of hyperparameters by focusing on promising regions of the parameter space, thus speeding up the optimization process.\n\n- **Hyperband Pruner**: This is a pruning algorithm that terminates unpromising trials early, allowing the optimizer to allocate more resources to promising candidates. It's particularly useful when the training process is computationally expensive.\n\n\n- **Logloss**: Logloss is used as an objective function for binary classification models. It measures the uncertainty of predictions, penalizing both wrong predictions and those that are overly confident. Minimizing logloss helps in building models that are well-calibrated.\n\n- **Early Stopping**: This is a technique where training is stopped when the model's performance on a validation set ceases to improve after a certain number of rounds. It prevents overfitting by not allowing the model to train for too long.\n\n\nEach of these components is critical to the model optimization process. By understanding and utilizing these terms effectively, you can drive your ML models to achieve superior performance, making them more reliable and impactful in practical applications.\n","metadata":{}},{"cell_type":"markdown","source":"For more details about optuna take a visit to it's [Documentation](https://optuna.readthedocs.io/en/stable/)","metadata":{}},{"cell_type":"code","source":"# This class is optimizing the models on weighted recall score which is defined below\n# weighted recall = 0.65 * recall + 0.35 * f1 \n# I am adding various evaluation metrics like recall,precision,f1,weighted_recall,accuracy to evaluate better\n\nclass ModelOptimizer:\n    def __init__(self, train_set, test_set):\n        self.train_set = train_set\n        self.test_set = test_set\n    \n    def cat_objective(self, trial):\n        param = {\n            'objective': 'Logloss',  # Set objective to Logloss\n            'eval_metric': 'AUC',  # Use AUC as evaluation metric\n            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),  # Suggest learning rate from log-uniform distribution\n            'depth': trial.suggest_int('depth', 3, 12),  # Suggest tree depth\n            'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-8, 10.0),  # Suggest L2 regularization parameter\n            'bagging_temperature': trial.suggest_uniform('bagging_temperature', 0.0, 1.0),  # Suggest bagging temperature\n            'border_count': trial.suggest_int('border_count', 1, 255),  # Suggest border count\n            'scale_pos_weight': trial.suggest_loguniform('scale_pos_weight', 1e-3, 1.0),  # Suggest scale positive weight\n            'verbose': 0,\n            'early_stopping_rounds': 200,  # Set early stopping rounds\n            'iterations': 3000  # Set maximum number of iterations\n        }\n\n        train_data = cb.Pool(data=self.train_set.drop('Churn', axis=1), label=self.train_set['Churn'])  # Prepare training data\n        test_data = cb.Pool(data=self.test_set.drop('Churn', axis=1), label=self.test_set['Churn'])  # Prepare testing data\n\n        cat = cb.CatBoostClassifier(**param)  # Initialize CatBoostClassifier with suggested parameters\n        cat.fit(train_data, eval_set=test_data, use_best_model=True)  # Train the model with early stopping\n\n        preds = cat.predict_proba(self.test_set.drop('Churn', axis=1))[:, 1]  # Get prediction probabilities\n        preds_digits = [1 if pred >= 0.4 else 0 for pred in preds]  # Convert probabilities to binary predictions\n        roc_auc = roc_auc_score(self.test_set['Churn'], preds)  # Calculate ROC AUC score\n        f1 = f1_score(self.test_set['Churn'], preds_digits)  # Calculate F1 score\n        recall = recall_score(self.test_set['Churn'], preds_digits)  # Calculate recall score\n        accuracy = accuracy_score(self.test_set['Churn'], preds_digits)  # Calculate accuracy score\n        weighted_recall = self.weighted_recall(self.test_set['Churn'], preds_digits)  # Calculate weighted recall score\n        prec = precision_score(self.test_set['Churn'], preds_digits)  # Calculate precision score\n        trial.set_user_attr('roc', roc_auc)  # Log ROC AUC score in Optuna trial attributes\n        trial.set_user_attr('f1', f1)  # Log F1 score in Optuna trial attributes\n        trial.set_user_attr('accuracy', accuracy)  # Log accuracy score in Optuna trial attributes\n        trial.set_user_attr('recall', recall)  # Log recall score in Optuna trial attributes\n        trial.set_user_attr('precision', prec)  # Log precision score in Optuna trial attributes\n        return weighted_recall  # Return weighted recall score for optimization\n\n    def optimize_catboost(self, n_trials=100):\n        cat_study = optuna.create_study(direction='maximize',\n                                        sampler=optuna.samplers.TPESampler(),  # Use TPE sampler for optimization\n                                        pruner=optuna.pruners.HyperbandPruner(\n                                            min_resource=5,  # Minimum resource for pruning\n                                            max_resource=10,  # Maximum resource for pruning\n                                            reduction_factor=2  # Reduction factor for pruning\n                                        ))\n        start_time = time.time()  # Record start time\n        cat_study.optimize(self.cat_objective, n_trials=n_trials , n_jobs=-1)  # Start optimization\n        print(f'time taken is {time.time() - start_time}')  # Print time taken for optimization\n        cat_study_df = cat_study.trials_dataframe()  # Convert study trials to DataFrame\n        return cat_study_df  # Return DataFrame with trial results\n\n    def nn_objective(self, trial):\n        model = Sequential()  # Initialize a Sequential model\n        model.add(Dense(trial.suggest_int('units_layer1', 32, 512), input_dim=self.train_set.drop('Churn', axis=1).shape[1]))  # Add first dense layer with suggested units\n        model.add(LeakyReLU(alpha=0.01))  # Add LeakyReLU activation\n        model.add(Dropout(trial.suggest_uniform('dropout_layer1', 0.2, 0.5)))  # Add Dropout with suggested rate\n\n        model.add(Dense(trial.suggest_int('units_layer2', 32, 512)))  # Add second dense layer with suggested units\n        model.add(LeakyReLU(alpha=0.01))  # Add LeakyReLU activation\n        model.add(Dropout(trial.suggest_uniform('dropout_layer2', 0.2, 0.5)))  # Add Dropout with suggested rate\n\n        model.add(Dense(1, activation='sigmoid'))  # Add output layer with sigmoid activation\n\n        learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)  # Suggest learning rate from log-uniform distribution\n        optimizer = Adam(learning_rate=learning_rate)  # Use Adam optimizer with suggested learning rate\n\n        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])  # Compile the model with binary crossentropy loss\n\n        model.fit(self.train_set.drop('Churn', axis=1), self.train_set['Churn'],\n                  validation_data=(self.test_set.drop('Churn', axis=1), self.test_set['Churn']),\n                  batch_size=trial.suggest_int('batch_size', 32, 128),  # Suggest batch size\n                  epochs=50,  # Set number of epochs\n                  callbacks=[tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)],  # Use early stopping\n                  verbose=0)\n\n        preds = model.predict(self.test_set.drop('Churn', axis=1)).ravel()  # Get prediction probabilities\n        preds_digits = [1 if pred >= 0.4 else 0 for pred in preds]  # Convert probabilities to binary predictions\n        roc_auc = roc_auc_score(self.test_set['Churn'], preds)  # Calculate ROC AUC score\n        f1 = f1_score(self.test_set['Churn'], preds_digits)  # Calculate F1 score\n        recall = recall_score(self.test_set['Churn'], preds_digits)  # Calculate recall score\n        accuracy = accuracy_score(self.test_set['Churn'], preds_digits)  # Calculate accuracy score\n        weighted_recall = self.weighted_recall(self.test_set['Churn'], preds_digits)  # Calculate weighted recall score\n        prec = precision_score(self.test_set['Churn'], preds_digits)  # Calculate precision score\n        trial.set_user_attr('roc', roc_auc)  # Log ROC AUC score in Optuna trial attributes\n        trial.set_user_attr('f1', f1)  # Log F1 score in Optuna trial attributes\n        trial.set_user_attr('accuracy', accuracy)  # Log accuracy score in Optuna trial attributes\n        trial.set_user_attr('recall', recall)  # Log recall score in Optuna trial attributes\n        trial.set_user_attr('precision', prec)  # Log precision score in Optuna trial attributes\n        return weighted_recall  # Return weighted recall score for optimization\n\n    def optimize_nn(self, n_trials=100):\n        nn_study = optuna.create_study(direction='maximize',\n                                       sampler=optuna.samplers.TPESampler(),  # Use TPE sampler for optimization\n                                       pruner=optuna.pruners.HyperbandPruner(\n                                           min_resource=5,  # Minimum resource for pruning\n                                           max_resource=20,  # Maximum resource for pruning\n                                           reduction_factor=2  # Reduction factor for pruning\n                                       ))\n        start_time = time.time()  # Record start time\n        nn_study.optimize(self.nn_objective, n_trials=n_trials , n_jobs=-1)  # Start optimization\n        print(f'time taken is {time.time() - start_time}')  # Print time taken for optimization\n        nn_study_df = nn_study.trials_dataframe()  # Convert study trials to DataFrame\n        return nn_study_df  # Return DataFrame with trial results\n    \n    def weighted_recall(self, y_true, y_pred):\n        recall = recall_score(y_true, y_pred)  # Calculate recall score\n        f1 = f1_score(y_true, y_pred)  # Calculate F1 score\n        weighted_metric = 0.65 * recall + 0.35 * f1  # Calculate weighted recall score\n        return weighted_metric  # Return weighted recall score\n    \n    def lgb_objective(self, trial):\n        param = {\n            'objective': 'binary',  # Set objective to binary classification\n            'metric': 'auc',  # Use AUC as evaluation metric\n            'verbosity': -1,\n            'boosting_type': 'gbdt',  # Use GBDT boosting type\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),  # Suggest L1 regularization parameter\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),  # Suggest L2 regularization parameter\n            'num_leaves': trial.suggest_int('num_leaves', 2, 256),  # Suggest number of leaves\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),  # Suggest feature fraction\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),  # Suggest bagging fraction\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),  # Suggest bagging frequency\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),  # Suggest minimum child samples\n            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),  # Suggest learning rate from log-uniform distribution\n        }\n        dtrain = lgb.Dataset(self.train_set.drop('Churn', axis=1), label=self.train_set['Churn'])  # Prepare training data\n        dvalid = lgb.Dataset(self.test_set.drop('Churn', axis=1), label=self.test_set['Churn'])  # Prepare validation data\n\n        gbm = lgb.train(param, dtrain, valid_sets=[dvalid], num_boost_round=10000,  # Train model with early stopping\n                        early_stopping_rounds=100, verbose_eval=False)\n\n        preds = gbm.predict(self.test_set.drop('Churn', axis=1))  # Get prediction probabilities\n        preds_digits = [1 if pred >= 0.4 else 0 for pred in preds]  # Convert probabilities to binary predictions\n        roc_auc = roc_auc_score(self.test_set['Churn'], preds)  # Calculate ROC AUC score\n        f1 = f1_score(self.test_set['Churn'], preds_digits)  # Calculate F1 score\n        recall = recall_score(self.test_set['Churn'], preds_digits)  # Calculate recall score\n        accuracy = accuracy_score(self.test_set['Churn'], preds_digits)  # Calculate accuracy score\n        weighted_recall = self.weighted_recall(self.test_set['Churn'], preds_digits)  # Calculate weighted recall score\n        prec = precision_score(self.test_set['Churn'], preds_digits)  # Calculate precision score\n        trial.set_user_attr('roc', roc_auc)  # Log ROC AUC score in Optuna trial attributes\n        trial.set_user_attr('f1', f1)  # Log F1 score in Optuna trial attributes\n        trial.set_user_attr('accuracy', accuracy)  # Log accuracy score in Optuna trial attributes\n        trial.set_user_attr('recall', recall)  # Log recall score in Optuna trial attributes\n        trial.set_user_attr('precision', prec)  # Log precision score in Optuna trial attributes\n        return weighted_recall  # Return weighted recall score for optimization\n\n    def optimize_lgb(self, n_trials=100):\n        lgb_study = optuna.create_study(direction='maximize',\n                                        sampler=optuna.samplers.TPESampler(),  # Use TPE sampler for optimization\n                                        pruner=optuna.pruners.HyperbandPruner(\n                                            min_resource=5,  # Minimum resource for pruning\n                                            max_resource=100,  # Maximum resource for pruning\n                                            reduction_factor=2  # Reduction factor for pruning\n                                        ))\n        start_time = time.time()  # Record start time\n        lgb_study.optimize(self.lgb_objective, n_trials=n_trials , n_jobs=-1)  # Start optimization\n        print(f'time taken is {time.time() - start_time}')  # Print time taken for optimization\n        lgb_study_df = lgb_study.trials_dataframe()  # Convert study trials to DataFrame\n        return lgb_study_df  # Return DataFrame with trial results\n\n    def xgb_objective(self, trial):\n        param = {\n            'objective': 'binary:logistic',\n            'eval_metric': 'auc',\n            'verbosity': 0,\n            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n            'max_depth': trial.suggest_int('max_depth', 3, 12),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n            'gamma': trial.suggest_loguniform('gamma', 1e-8, 10.0),\n            'lambda': trial.suggest_loguniform('lambda', 1e-8, 10.0),\n            'alpha': trial.suggest_loguniform('alpha', 1e-8, 10.0),\n            'subsample': trial.suggest_uniform('subsample', 0.4, 1.0),\n            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.4, 1.0),\n            'early_stopping_rounds': 200,\n            'num_boost_round':3000,\n        }\n\n        train_data = xgb.DMatrix(self.train_set.drop('Churn', axis=1), label=self.train_set['Churn'])\n        test_data = xgb.DMatrix(self.test_set.drop('Churn', axis=1), label=self.test_set['Churn'])\n\n        gbm = xgb.train(param, train_data, evals=[(test_data, 'eval')], verbose_eval=False)\n\n        preds = gbm.predict(test_data)\n        preds_digits = [1 if pred >= 0.4 else 0 for pred in preds]\n        roc_auc = roc_auc_score(self.test_set['Churn'], preds)\n        f1 = f1_score(self.test_set['Churn'],preds_digits)\n        recall = recall_score(self.test_set['Churn'],preds_digits)\n        accuracy = accuracy_score(self.test_set['Churn'],preds_digits)\n        weighted_recall = self.weighted_recall(self.test_set['Churn'],preds_digits)\n        prec = precision_score(self.test_set['Churn'],preds_digits)\n        trial.set_user_attr('roc',roc_auc)\n        trial.set_user_attr('f1',f1)\n        trial.set_user_attr('accuracy',accuracy)\n        trial.set_user_attr('recall',recall)\n        trial.set_user_attr('precision',prec)\n        return weighted_recall\n\n    def optimize_xgb(self, n_trials=100):\n        xgb_study = optuna.create_study(direction='maximize',\n                                        sampler=optuna.samplers.TPESampler(),\n                                        pruner=optuna.pruners.HyperbandPruner(\n                                            min_resource=5,\n                                            max_resource=20,\n                                            reduction_factor=2\n                                        ))\n        start_time = time.time()\n        xgb_study.optimize(self.xgb_objective, n_trials=n_trials , n_jobs = -1)\n        print(f'time taken is {time.time() - start_time}')\n        xgb_study_df = xgb_study.trials_dataframe()\n        return xgb_study_df","metadata":{"_uuid":"1f92c032-dfae-4b71-ae0f-fad88c684c3d","_cell_guid":"cbf007b1-502d-45b7-87a9-cfb57cff82f8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save different results to kaggle working directory\ndef save_results(study_dfs, file_paths):\n    for df, path in zip(study_dfs, file_paths):\n        df.to_csv(path)\n        shutil.move(path, f'/kaggle/working/{path}')\n\n# load the results \ndef load_results(file_paths):\n    return [pd.read_csv(path).drop('Unnamed: 0', axis=1).reset_index(drop=True) for path in file_paths]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" let's optimize the models using the engineered dataset and oiginal(without engineered)\n I am not runnig the below code as i have already optimized and saved the result if you want to run the code please change the variables **run_featured_trials,run_org_trials** to ***True***","metadata":{"_uuid":"8c5b6174-e85f-40ae-bc65-f95bd2faccd2","_cell_guid":"0082a9f5-7752-46c2-9c77-f847d4928616","trusted":true}},{"cell_type":"markdown","source":"### ***Warning*** :- It may take few  Hours to run trials.","metadata":{}},{"cell_type":"code","source":"run_featured_trials = False\nrun_org_trials = False","metadata":{"_uuid":"3cee0bc1-3c94-45a8-a31d-b908a029b466","_cell_guid":"122e9528-97e6-41ee-8987-c614c75a2926","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if run_featured_trials:\n    optimizer = ModelOptimizer(transformed_featured_train_set,transformed_featured_val_set)\n    featured_xgb_study = optimizer.optimize_xgb()\n    featured_cat_study = optimizer.optimize_catboost()\n    featured_lgb_study = optimizer.optimize_lgb()\n    featured_nn_study = optimizer.optimize_nn()\nelse:\n    # loading the study which was done using featured dataset\n    featured_lgb_study , featured_xgb_study , featured_cat_study ,featured_nn_study = load_results([\"/kaggle/input/weighted-recall-reports/weighted_recall_lgb_study.csv\",\"/kaggle/input/weighted-recall-reports/weighted_recall_xgb_study(1).csv\",\"/kaggle/input/weighted-recall-reports/weighted_recall_cat_study(1).csv\",\"/kaggle/input/weighted-recall-reports/weighted_recall_nn_study.csv\"])","metadata":{"_uuid":"a1c2dbf6-a07b-40ef-820b-35c06228782c","_cell_guid":"3f1daa96-990f-4c5c-9458-054dbbe8cb13","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if run_org_trials:\n    transfm  = FeatureTransformer(train_set_splitted,val_set)\n    transformed_train_set,transformed_test_set = transfm.transform()\n    org_optimizer = ModelOptimizer(transformed_featured_train_set,transformed_featured_val_set)\n    org_xgb_study = org_optimizer.optimize_xgb()\n    org_cat_study = org_optimizer.optimize_catboost()\n    org_lgb_study = org_optimizer.optimize_lgb()\n    org_nn_study = org_optimizer.optimize_nn()\nelse:\n    \n    \n    # loading the study which was done using original dataset\n    org_lgb_study , org_xgb_study,org_cat_study,org_nn_study = load_results([\"/kaggle/input/orgs-report/lgb_org_report.csv\",\"/kaggle/input/orgs-report/xgb_org_report.csv\",\"/kaggle/input/orgs-report/catboost_org_report.csv\",\"/kaggle/input/orgs-report/nn_org_study.csv\"])","metadata":{"_uuid":"d910cb77-3e81-4955-98ce-b174acf7759b","_cell_guid":"04e6e4ef-fbd2-45a8-9474-658ca253bba5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you have any doubts regarding optimzation and feature engineering [visit Q&A](#qa-section) and if not let's see the results of the studies.","metadata":{}},{"cell_type":"code","source":"featured_lgb_study.iloc[featured_lgb_study['user_attrs_recall'].idxmax()]","metadata":{"_uuid":"e5855958-3b29-451f-8c74-d9b151bbd17c","_cell_guid":"e1c3ee47-739e-40e7-8569-11010c1ee992","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"featured_nn_study.iloc[featured_nn_study['user_attrs_recall'].idxmax()]","metadata":{"_uuid":"ac4f7f20-59fd-4086-9de7-e15d751c817d","_cell_guid":"f705f059-7f84-4294-8b95-89f99b456bb9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"featured_cat_study.iloc[featured_cat_study['user_attrs_recall'].idxmax()]","metadata":{"_uuid":"741094ca-d16b-44ce-98ca-b3dcfb17ec0b","_cell_guid":"54e0e472-4802-4d08-9dbc-5722cf7da034","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"featured_xgb_study.iloc[featured_xgb_study['user_attrs_recall'].idxmax()]","metadata":{"_uuid":"1a035404-91bc-4b9b-930c-de659baf02ae","_cell_guid":"fb327f45-c386-420c-bd6b-e88b8c0d52bf","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Results are imporved**.You can explore more as per your wish.","metadata":{"_uuid":"c6983122-92a8-4b2e-a249-d9c56b635c0b","_cell_guid":"23f6a664-60ee-4dc7-b462-fd78c3343fe0","trusted":true}},{"cell_type":"code","source":"# this function drop the columns which is not parameters from the study \ndef drop_unnessesary_columns(df):\n    drop_cols = []\n    for column in df.columns:\n        if 'params' not in column:\n            drop_cols.append(column)\n    df = df.drop(columns=drop_cols)\n    return df","metadata":{"_uuid":"21efee47-2693-48f5-bcef-2387d5d6ada1","_cell_guid":"4f31d11e-541b-41c3-82f4-2e05d2269fca","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_hyperparameters(hyperparameters):\n    clean_params = {}\n    for key, value in hyperparameters.items():\n        clean_key = key.replace('params_', '')  # Remove 'params_' prefix from the key\n        if isinstance(value, float) and value.is_integer():  # Check if float value is actually an integer\n            clean_params[clean_key] = int(value)  # Convert to integer if float is an integer\n        else:\n            clean_params[clean_key] = value  # Otherwise, keep the value as it is\n    return clean_params  # Return the cleaned hyperparameters dictionary","metadata":{"_uuid":"fb0fd764-9060-4925-a8b5-862d23ff101b","_cell_guid":"9b2ef777-0abe-4eb1-8dbb-fe7467c7ee00","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling  and optimization","metadata":{"_uuid":"aa9543ec-171f-40c5-bba5-f9aa81e3768f","_cell_guid":"edc4eba4-24a6-4fa4-a969-15aeafe89fe7","trusted":true}},{"cell_type":"markdown","source":"Time for a group project! Hopefully, \nthis ensemble model doesn‚Äôt have that one lazy model that doesn‚Äôt pull its weight. Looking at you, CatBoost...\n","metadata":{}},{"cell_type":"markdown","source":"# üåü **Unleashing the Power of Optimization: The Art of Model Tuning** üåü\n\nWelcome to the exciting world of model optimization! Today, we're diving into a sophisticated process where we harness the full potential of multiple models and fine-tune their performance to achieve the best results. \n\n1. **Sampling Hyperparameters**: We sample indices from the study dataframes for different models (LightGBM, XGBoost, CatBoost, and Neural Network). Each index corresponds to a unique set of hyperparameters. By doing this, we explore various configurations and their impact on model performance.\n\n2. **Fetching and Cleaning Parameters**: Once we have the indices, we retrieve the corresponding hyperparameters for each model. We then clean these parameters to ensure they are ready for training. This process is akin to preparing your ingredients before cooking‚Äîmaking sure they are in their best form.\n\n3. **Setting Fixed Parameters**: Besides the sampled hyperparameters, we set some fixed parameters to control aspects like verbosity or early stopping. These are like the constants in our recipe that don‚Äôt change but are crucial for consistency.\n\n### ü§ù **Combining Model Predictions: The Magic of Weighted Voting** ü§ù\n\nAfter training our models with the selected hyperparameters, we combine their predictions using weighted voting. This is where the real magic happens:\n\n- **Training Models**: Each model (LightGBM, XGBoost, CatBoost, and Neural Network) is trained on the training data and then used to make predictions on the validation data.\n  \n- **Weighting Predictions**: We assign weights to each model‚Äôs predictions based on their performance. Think of this as adjusting the volume of each instrument in our orchestra to get the perfect balance.\n\n- **Combining Predictions**: Using these weights, we combine the predictions from all models. This combined prediction is like a beautifully orchestrated piece of music, where each model's output contributes to the final performance.\n\n- **Evaluating Performance**: We calculate various performance metrics such as ROC AUC, F1 score, recall, and accuracy to evaluate the effectiveness of our combined predictions. This helps us understand how well our ensemble performs and guides us in refining our approach.\n\n### üîß **Why This Matters** üîß\n\nThis optimization process ensures that we are not just relying on a single model but leveraging the strengths of multiple models. By tuning hyperparameters and combining predictions smartly, we maximize our model‚Äôs performance and enhance its ability to make accurate predictions.\n\nSo, buckle up and get ready to see how this intricate dance of model optimization and prediction combination works its magic in achieving top-notch performance!\n","metadata":{}},{"cell_type":"code","source":"\ndef weighted_voting(predictions, weights):\n    combined_predictions = np.zeros_like(next(iter(predictions.values())))  # Initialize array for combined predictions\n    for model_name, model_preds in predictions.items():\n        combined_predictions += weights[model_name] * model_preds  # Weight and sum predictions from each model\n    combined_predictions /= sum(weights.values())  # Normalize the combined predictions by the sum of weights\n    return combined_predictions  # Return the final weighted prediction\n\ndef objective(trial):\n    X_train, y_train = transformed_featured_train_set.drop('Churn', axis=1), transformed_featured_train_set['Churn']  # Extract training features and labels\n    X_val, y_val = transformed_featured_val_set.drop('Churn', axis=1), transformed_featured_val_set['Churn']  # Extract validation features and labels\n    \n    # Sample indices for hyperparameters from different models\n    lgb_study = drop_unnecessary_columns(featured_lgb_study.copy())  # Prepare LightGBM study data\n    xgb_study = drop_unnecessary_columns(featured_xgb_study.copy())  # Prepare XGBoost study data\n    cat_study = drop_unnecessary_columns(featured_cat_study.copy())  # Prepare CatBoost study data\n    nn_study = drop_unnecessary_columns(featured_nn_study.copy())  # Prepare Neural Network study data\n    \n    lgb_idx = trial.suggest_int('lgb_idx', 0, len(lgb_study) - 1)  # Sample index for LightGBM hyperparameters\n    xgb_idx = trial.suggest_int('xgb_idx', 0, len(xgb_study) - 1)  # Sample index for XGBoost hyperparameters\n    cat_idx = trial.suggest_int('cat_idx', 0, len(cat_study) - 1)  # Sample index for CatBoost hyperparameters\n    nn_idx = trial.suggest_int('nn_idx', 0, len(nn_study) - 1)  # Sample index for Neural Network hyperparameters\n    \n    # Fetch hyperparameters and clean them\n    lgb_params = clean_hyperparameters(lgb_study.iloc[lgb_idx].to_dict())  # Get LightGBM parameters\n    xgb_params = clean_hyperparameters(xgb_study.iloc[xgb_idx].to_dict())  # Get XGBoost parameters\n    cat_params = clean_hyperparameters(cat_study.iloc[cat_idx].to_dict())  # Get CatBoost parameters\n    nn_params = clean_hyperparameters(nn_study.iloc[nn_idx].to_dict())  # Get Neural Network parameters\n    \n    # Set additional fixed hyperparameters\n    lgb_params['verbose'] = -1  # Silence LightGBM output\n    xgb_params['verbose'] = 0  # Silence XGBoost output\n    cat_params['early_stopping_rounds'] = 3000  # Set early stopping rounds for CatBoost\n    cat_params['iterations'] = 200  # Set number of iterations for CatBoost\n\n    # Weights for the voting classifier\n    lgb_weight = trial.suggest_float('lgb_weight', 0.1, 1.0)  # Suggest weight for LightGBM\n    xgb_weight = trial.suggest_float('xgb_weight', 0.1, 1.0)  # Suggest weight for XGBoost\n    cat_weight = trial.suggest_float('cat_weight', 0.1, 1.0)  # Suggest weight for CatBoost\n    nn_weight = trial.suggest_float('nn_weight', 0.1, 1.0)  # Suggest weight for Neural Network\n\n    weights = {\n        'lgb': lgb_weight,\n        'xgb': xgb_weight,\n        'cat': cat_weight,\n        'nn': nn_weight\n    }  # Store weights in a dictionary\n    \n    #Let‚Äôs give these models some workout time. First up, LightGBM. \n    #It's like the gym but for data‚Äîlet‚Äôs see if it can lift those predictions high!\n    \n    # Train and predict with LightGBM\n    lgb_train = lgb.Dataset(X_train, label=y_train)  # Prepare LightGBM dataset\n    lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=100)  # Train LightGBM model\n    lgb_preds = lgb_model.predict(X_val)  # Predict on validation set with LightGBM\n    \n    #Phew, LightGBM is done. Now let‚Äôs see if XGBoost can boost our mood... or just our predictions!\"\n    \n    # Train and predict with XGBoost\n    xgb_model = xgb.XGBClassifier(**xgb_params)  # Initialize XGBoost model with parameters\n    xgb_model.fit(X_train, y_train)  # Train XGBoost model\n    xgb_preds = xgb_model.predict_proba(X_val)[:, 1]  # Predict on validation set with XGBoost and get probabilities\n\n    # Train and predict with CatBoost\n    cat_model = cb.CatBoostClassifier(**cat_params, verbose=0)  # Initialize CatBoost model with parameters\n    cat_model.fit(X_train, y_train)  # Train CatBoost model\n    cat_preds = cat_model.predict_proba(X_val)[:, 1]  # Predict on validation set with CatBoost and get probabilities\n\n    # Train and predict with Neural Network using TensorFlow/Keras\n    nn_model = create_nn_model(nn_params, transformed_featured_train_set.shape[1] - 1)  # Create Neural Network model\n    nn_model.fit(X_train, y_train, epochs=50, batch_size=int(nn_params['batch_size']), verbose=0)  # Train Neural Network model\n    nn_preds = nn_model.predict(transformed_featured_val_set.drop('Churn', axis=1)).ravel()  # Predict on validation set with Neural Network\n\n    # Combine predictions using weighted soft voting\n    predictions = {\n        'lgb': lgb_preds,\n        'xgb': xgb_preds,\n        'cat': cat_preds,\n        'nn': nn_preds\n    }  # Store predictions in a dictionary\n    combined_preds = weighted_voting(predictions, weights)  # Perform weighted voting to combine predictions\n    preds_digits = [1 if pred >= 0.4 else 0 for pred in combined_preds]  # Convert probabilities to binary predictions with a threshold of 0.4\n    \n    # Calculate evaluation metrics\n    roc_auc = roc_auc_score(y_val, combined_preds)  # Calculate ROC AUC score\n    f1 = f1_score(y_val, preds_digits)  # Calculate F1 score\n    recall = recall_score(y_val, preds_digits)  # Calculate recall score\n    accuracy = accuracy_score(y_val, preds_digits)  # Calculate accuracy score\n    weighted_recall = 0.65 * recall + 0.35 * f1  # Calculate weighted recall combining recall and F1 score\n    prec = precision_score(y_val, preds_digits)  # Calculate precision score\n    \n    # Store metrics as trial user attributes\n    trial.set_user_attr('roc', roc_auc)  # Store ROC AUC score in the study\n    trial.set_user_attr('f1', f1)  # Store F1 score in the study object\n    trial.set_user_attr('accuracy', accuracy)  # Store accuracy score\n    trial.set_user_attr('recall', recall)  # Store recall score\n    trial.set_user_attr('precision', prec)  # Store precision score\n    \n    return weighted_recall  # Return weighted recall as the objective value for optimization","metadata":{"_uuid":"c4b5546f-5575-4f86-879d-fc99ec66c601","_cell_guid":"e17a6ef6-c48d-468a-a698-3a6e8994e57b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's get ready to build a neural network model that will make your neurons fire with excitement! Or, at the very least, your CPU fan spin a little faster","metadata":{}},{"cell_type":"code","source":"def create_nn_model(params, input_shape):\n    model = Sequential()\n    model.add(Dense(params['units_layer1'], input_dim=input_shape, activation='relu'))  # First dense layer\n    model.add(Dropout(params['dropout_layer1']))  # Dropout layer after first dense layer\n    model.add(Dense(params['units_layer2'], activation='relu'))  # Second dense layer\n    model.add(Dropout(params['dropout_layer2']))  # Dropout layer after second dense layer\n    model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification\n    optimizer = Adam(learning_rate=params['learning_rate'])  # Adam optimizer with specified learning rate\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])  # Compile model with binary crossentropy loss and AUC metric\n    return model  # Return the compiled model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you want to run the optimzation on ensemble than you should change the variable **run_ensemble_trials** to ***True***","metadata":{"_uuid":"79d3ca6b-64d6-4782-bebb-e36e917ea745","_cell_guid":"4a0a3f57-6497-46b7-9814-a6b5ba0cfeb2","trusted":true}},{"cell_type":"code","source":"run_ensemble_trials = False","metadata":{"_uuid":"307f2e0d-ec18-4040-9ad2-7ed1bb3146d5","_cell_guid":"009bfb88-424e-45f6-b997-17330b806e7b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if run_ensemble_trials:\n    # Create Optuna study and optimize\n    study = optuna.create_study(direction='maximize')\n    study.optimize(objective, n_trials=25 , n_jobs =-1)\n\n    # Best trial\n    best_trial = study.best_trial\n    print(f'Best metric: {best_trial.value}')\n    print('Best hyperparameters and weights:', best_trial.params)\n    trials = study.trials\n\n    # Extract trial data\n    data = {\n        'trial_number': [trial.number for trial in trials],\n        'value': [trial.value for trial in trials],\n        'params': [trial.params for trial in trials],\n        'datetime_start': [trial.datetime_start for trial in trials],\n        'datetime_complete': [trial.datetime_complete for trial in trials],\n        'f1': [trial.user_attrs.get('f1', None) for trial in trials],\n        'accuracy': [trial.user_attrs.get('accuracy', None) for trial in trials],\n        'roc': [trial.user_attrs.get('roc', None) for trial in trials],\n        'recall': [trial.user_attrs.get('recall', None) for trial in trials],\n        'precision': [trial.user_attrs.get('precision', None) for trial in trials]\n        \n    }\n\n    # Convert to DataFrame\n    ensemble_results_df = pd.DataFrame(data)\n    \nelse:\n    ensemble_results_df = pd.read_csv(\"/kaggle/input/ensemble-study/ensemble_opt_study.csv\")\n    ensemble_results_df = ensemble_results_df.drop(columns=\"Unnamed: 0\")","metadata":{"_uuid":"419fce55-bffc-4dee-9ae3-2df96e4ae79e","_cell_guid":"aee4f755-9148-41b8-bf17-18606161f9c5","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model's performance which has highest weighted recall\nensemble_results_df.iloc[ensemble_results_df['value'].idxmax()]","metadata":{"_uuid":"bda8d89c-4d05-4ecb-be78-5ddd4e84ae18","_cell_guid":"f73cf100-b353-4481-af00-1432a12dfb87","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model's performance which has highest recall\nensemble_results_df.iloc[ensemble_results_df['recall'].idxmax()]","metadata":{"_uuid":"d2a604a3-7f0f-4e09-8f9a-3e9683020c96","_cell_guid":"d758eba7-5796-45e2-9652-e4930ff286f5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model's performance which has highest precision\nensemble_results_df.iloc[ensemble_results_df['precision'].idxmax()]","metadata":{"_uuid":"e0547be1-45a9-4e53-a282-a341ee792f7f","_cell_guid":"d4c244c8-81a4-4aa4-b159-a6bd27ad6d5a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Well, that was like organizing a party where everyone brought the same dish‚Äîlooks like we might be better off just sticking with one good chef!.\n Result is good compare to unoptimized model but it is not good compare to optimized ones.","metadata":{"_uuid":"15079a3b-1be7-44ea-927a-e6055eac6170","_cell_guid":"6178c278-16af-4c19-bc5e-5044d89e9102","trusted":true}},{"cell_type":"code","source":"def get_predictions(models, X):\n    predictions = []\n    for model in models:\n        pred = model.predict(X).ravel()\n        predictions.append(pred)\n    return predictions\n\n# Function to perform soft voting\ndef soft_voting(models, X):\n    predictions = get_predictions(models, X)\n    avg_predictions = np.mean(predictions, axis=0)\n    return avg_predictions","metadata":{"_uuid":"56acb885-4c15-4ae2-90b1-41726011bfcf","_cell_guid":"197bf000-4ddf-4756-9c25-18c836cd46d4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After much deliberation, and possibly a few cups of coffee, we‚Äôve decided to stick with the one model that didn‚Äôt disappoint us‚ÄîANN. Sometimes, it‚Äôs better to have one friend you can rely on than a group of unreliable ones!","metadata":{"_uuid":"8ee3520d-9d52-4f65-b593-902fd2805c49","_cell_guid":"56570230-f459-4820-a99b-6b4d3763eb74","trusted":true}},{"cell_type":"markdown","source":"The best one is ANN at index 87","metadata":{}},{"cell_type":"code","source":"ann_model = create_nn_model(clean_hyperparameters(featured_nn_study.iloc[87].to_dict()),input_shape=transformed_featured_train_set.drop(columns='Churn').shape[1])\nann_model.fit(transformed_featured_final_train_set.drop(columns='Churn'),transformed_featured_final_train_set['Churn'],epochs=20,validation_data=(transformed_featured_test_set.drop(columns='Churn'),transformed_featured_test_set['Churn']))","metadata":{"_uuid":"fd4e87ce-16e2-4d4f-9c4c-23c2a21168ec","_cell_guid":"0275a7ca-5de3-412d-a562-3d5f6e3d21ee","collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"Well, look at that! Our ANN model is performing like a champ! Who knew a bunch of neurons could be this reliable? Let‚Äôs give it a round of applause (and maybe save it before it gets too proud of itself).","metadata":{}},{"cell_type":"code","source":"#let's see final model's performance\npreds = ann_model.predict(transformed_featured_test_set.drop(columns='Churn'))\nprint('roc_auc score is:-',roc_auc_score(transformed_featured_test_set['Churn'],preds))\npreds_digits = [1 if pred >= 0.4 else 0 for pred in preds]\nprint('recall score is :-',recall_score(transformed_featured_test_set['Churn'],preds_digits))\nprint('precision score is:-',precision_score(transformed_featured_test_set['Churn'],preds_digits))\nann_model.save('ann_model_for_customer_churn.h5')","metadata":{"_uuid":"af0c5ec3-868b-4861-b3fa-7db2497951a8","_cell_guid":"f185fca4-eea8-4c50-91dc-a854e0647022","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And there you have it! We‚Äôve churned through the data, boosted our spirits, and even made some neural connections along the way. Thanks for sticking around‚Äîyour presence was more than 0.4 (binary threshold humor, anyone?)! Until next time, keep predicting and stay curious!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"qa-section\"></a>\n## Q&A: for this Notebook\n\n**1. Why did I choose XGBoost, LightGBM, ANN, and CatBoost?**\n- **Answer:** These models are known for their robustness and ability to deliver high performance with relatively quick training times. They are widely used for classification tasks and handle various types of data and feature complexities effectively.\n\n**2. Why did I write many classes and functions?**\n- **Answer:** To ensure reproducibility and maintain a well-organized workflow. By encapsulating code in functions and classes, the project becomes more modular, easier to debug, and scalable for future enhancements.\n\n**3. Why did I use the Featuretools library for feature engineering?**\n- **Answer:** Feature engineering is a crucial aspect of model performance. Since I lacked domain-specific knowledge in the telecom industry, an automated feature engineering approach like Featuretools was ideal. It enabled me to generate valuable features without requiring deep domain expertise, and I was impressed with the quality of the features it produced.\n\n**4. Why did I choose weighted recall as the evaluation criterion?**\n- **Answer:** In churn prediction, accurately identifying customers who are likely to churn (the positive class) is essential for taking preventive actions. Weighted recall emphasizes the importance of correctly identifying these customers, helping to optimize the model's performance for this specific task.\n\n**5. Why did I opt for sampling instead of assigning higher weights to the minority class?**\n- **Answer:** I wanted to experiment with how the models would perform without relying on class weights. In a different [notebook](https://www.kaggle.com/code/deepsutariya/churn-modeling-to-deployment-mlflow-DagsHub), I explored the impact of increased class weights, but here, I focused on understanding the models' behavior with sampling techniques.\n\n**6. Why did I choose to optimize the original dataset, SMOTEd datasets, and featured datasets?**\n- **Answer:** I was experimenting with different data configurations to assess how each impacted model performance. This approach allowed me to compare the effectiveness of original, SMOTEd, and feature-engineered datasets in driving model accuracy and generalization.\n\n**7. Why did I select ANN as the final model?**\n- **Answer:** After training various models, I initially attempted to ensemble them and optimize the ensemble. However, the ensemble approach didn't yield the desired results. I then selected the top 5 individual models and tried an equal-weight ensemble, but this also underperformed. Ultimately, I chose the best-performing individual model, which was the ANN, as the final model for the project.\n\n\n**8. Why did I use Optuna for hyperparameter optimization?**\n- **Answer:** Optuna is a powerful tool for hyperparameter tuning, Best part of the oputna is ***Parallal Training***. It helped me find the best-performing model configurations, ultimately improving the accuracy and reliability of the predictions.\n\n**9. What role did linear regression play in the final model selection?**\n- **Answer:** I used linear regression to capture the relationships between hyperparameters and model performance. This allowed me to predict additional hyperparameter combinations that might perform exceptionally well, further fine-tuning the final model.Unfortunatly , it didn't work. So i removed this part, If you want to experiment with that you can  visit old version (33) of this notebook.\n\n**10. Why did I use an ensemble of models instead of relying on a single model?**\n- **Answer:** Ensembling helps to combine the strengths of different models, potentially leading to more robust and accurate predictions. Although the ensemble didn't outperform the best individual model in this case, it was important to explore this option to ensure that no potential performance gains were overlooked.\n\n**11. How did I handle the class imbalance in the dataset?**\n- **Answer:** I experimented with both SMOTE and sampling techniques to balance the class distribution. This was crucial for ensuring that the model was not biased towards the majority class, thus improving its ability to correctly predict churners.\n\n**12.Where is the EDA section?**\n- **Answer:** It is in the first notebook of this series.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nCongratulations! You've just unlocked two incredibly powerful tools in the world of data science‚Äî**Featuretools** and **Optuna**. By mastering these, you've gained skills that aren't just theoretical but directly applicable to real-world problems and production environments. Imagine the countless hours saved by automating feature engineering with Featuretools, and the sheer precision added to your models through Optuna's hyperparameter optimization. These are skills that will serve you not just today, but for the rest of your career.\n\nThe knowledge you've acquired here is more than just another bullet point on your resume‚Äîit's a lifelong asset. Whether you're optimizing models for a small startup or deploying complex systems at scale, Featuretools and Optuna will be your go-to tools, helping you solve problems more efficiently and effectively.\n\n## What's Next?\n\nBut the journey doesn't end here. Imagine running hundreds of optimization studies in parallel, each one fine-tuning models to perfection. Now, think about the chaos of managing each of these experiments manually. Impossible, right?\n\nThis is where **MLflow** comes into play, and that's exactly what we'll dive into in the next [notebook](https://www.kaggle.com/code/deepsutariya/churn-modeling-to-deployment-mlflow-DagsHub). MLflow is the key to scaling your optimization efforts, allowing you to track, manage, and deploy your models with ease. It's like having a superpower in the world of machine learning‚Äîa way to bring order to the chaos of experimentation.\n\nSo, get ready to elevate your skills even further. The next notebook is not just a continuation‚Äîit's a leap into the future of machine learning at scale. Don't miss out on discovering how MLflow can make your life easier and your work more impactful. See you there!\n","metadata":{}}]}